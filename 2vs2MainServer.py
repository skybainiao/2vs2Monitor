import asyncio
import aiohttp
import json
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Tuple
import psycopg2
from psycopg2 import pool
from psycopg2.extras import DictCursor
import functools
import time
from collections import defaultdict, ChainMap
from hashlib import md5  # ç”¨äºæ•°æ®å“ˆå¸Œå¯¹æ¯”
# å¯¼å…¥WebSocketåº“
import websockets


# === é…ç½®åŒº ===
# APIé…ç½®ï¼ˆæ–°å¢ï¼šä¸¤ä¸ªsource1çš„APIï¼Œsource2å›ºå®šï¼‰
SOURCE1_URLS = [
  #"http://127.0.0.1:5001/get_odds1",
    "http://103.67.53.137:5001/get_odds1",
    "http://122.10.118.13:5001/get_odds1",
    "http://154.222.29.140:5001/get_odds1"

]
SOURCE2_URL = "http://127.0.0.1:5002/get_odds2"  # source2å›ºå®šAPI

# ä¿ç•™åŸAPI_URLSç»“æ„ï¼ˆåŠ¨æ€ç”Ÿæˆï¼Œç¡®ä¿å…¶ä»–é€»è¾‘å…¼å®¹ï¼‰
API_URLS = [SOURCE1_URLS[0], SOURCE2_URL]  # åˆå§‹å€¼ï¼Œåç»­ä¼šåŠ¨æ€æ›´æ–°

# æ•°æ®åº“é…ç½®
DB_CONFIG = {
    "host": "localhost",
    "database": "postgres",
    "user": "postgres",
    "password": "cjj2468830035",
    "port": 5432
}

# è¿æ¥æ± é…ç½®
DB_POOL_CONFIG = {
    "minconn": 2,  # æœ€å°è¿æ¥æ•°
    "maxconn": 50,  # æœ€å¤§è¿æ¥æ•°
    **DB_CONFIG  # ç»§æ‰¿åŸºç¡€æ•°æ®åº“é…ç½®
}

# é‡è¯•é…ç½®
MAX_RETRIES = 3
RETRY_DELAY = 2  # ç§’

# å®šæ—¶ä»»åŠ¡é…ç½®
FETCH_INTERVAL = 30  # ç§’

# APIå¤±æ•ˆå¤„ç†é…ç½®
API_FAILURE_DELAY = 60  # å¤±æ•ˆåæš‚åœæ—¶é—´ï¼ˆç§’ï¼‰

# === å…¨å±€å˜é‡ ===
postgres_pool = None  # æ•°æ®åº“è¿æ¥æ± 
last_matches_data = {}  # ä¸Šæ¬¡çš„æ¯”èµ›æ•°æ®ç¼“å­˜ {match_name: (hash, data)}

# æ–°å¢ï¼šsource1è½®æ¢ç›¸å…³
current_source1_index = 0  # è½®æ¢ç´¢å¼•ï¼ˆ0å’Œ1äº¤æ›¿ï¼‰

# === æ–°å¢ï¼šå…¨å±€æ¯”èµ›æ•°æ®ç¼“å­˜ ===
all_matches_cache = {}  # æ‰€æœ‰æ¯”èµ›çš„æœ€æ–°æ•°æ®ç¼“å­˜ {match_name: data}
current_api_errors = set()  # å­˜å‚¨å½“å‰å¤±è´¥çš„API URL


# === æ–°å¢ï¼šWebSocketç›¸å…³é…ç½® ===
WS_CONFIG = {
    "host": "160.25.20.18",
    "port": 8765
}

# å­˜å‚¨æ‰€æœ‰è¿æ¥çš„å®¢æˆ·ç«¯
connected_clients = set()

# === è£…é¥°å™¨ ===
def timed(func):
    """å‡½æ•°æ‰§è¡Œæ—¶é—´è£…é¥°å™¨"""

    @functools.wraps(func)
    async def wrapper(*args, **kwargs):
        start_time = time.perf_counter()
        result = await func(*args, **kwargs)
        end_time = time.perf_counter()
        print(f"â±ï¸ {func.__name__} æ‰§è¡Œæ—¶é—´: {(end_time - start_time) * 1000:.2f}ms")
        return result

    return wrapper


# === APIè¯·æ±‚æ¨¡å— ===
async def fetch_api(session, url, retries=MAX_RETRIES):
    """å¼‚æ­¥è·å–APIæ•°æ®ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
    for attempt in range(retries):
        start_time = datetime.now()
        try:
            async with session.get(url) as response:
                print(url)
                elapsed = (datetime.now() - start_time).total_seconds() * 1000  # æ¯«ç§’
                if response.status == 200:
                    data = await response.json(content_type=None)  # å¤„ç†éæ ‡å‡†JSONå“åº”
                    return {
                        "url": url,
                        "status": "success",
                        "data": data,
                        "timestamp": datetime.now().isoformat(),
                        "response_time": elapsed
                    }
        except Exception as e:
            pass
        if attempt < retries - 1:
            await asyncio.sleep(RETRY_DELAY)
    return {
        "url": url,
        "status": "error",
        "error_message": f"è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({retries})",
        "timestamp": datetime.now().isoformat(),
        "response_time": (datetime.now() - start_time).total_seconds() * 1000
    }


# === æ•°æ®åº“è¿æ¥æ± æ¨¡å— ===
def init_db_pool():
    """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± """
    global postgres_pool
    try:
        postgres_pool = pool.SimpleConnectionPool(**DB_POOL_CONFIG)
        print(
            f"âœ… æ•°æ®åº“è¿æ¥æ± åˆå§‹åŒ–æˆåŠŸï¼Œæœ€å°è¿æ¥æ•°: {DB_POOL_CONFIG['minconn']}ï¼Œæœ€å¤§è¿æ¥æ•°: {DB_POOL_CONFIG['maxconn']}")
        return True
    except Exception as e:
        print(f"âŒ æ•°æ®åº“è¿æ¥æ± åˆå§‹åŒ–å¤±è´¥: {e}")
        return False


def get_db_connection():
    """ä»è¿æ¥æ± è·å–æ•°æ®åº“è¿æ¥"""
    if postgres_pool is None:
        if not init_db_pool():
            return None
    try:
        return postgres_pool.getconn()
    except Exception as e:
        print(f"âŒ ä»è¿æ¥æ± è·å–è¿æ¥å¤±è´¥: {e}")
        return None


def release_db_connection(conn):
    """å°†æ•°æ®åº“è¿æ¥é‡Šæ”¾å›è¿æ¥æ± """
    if conn and postgres_pool:
        try:
            postgres_pool.putconn(conn)
        except Exception as e:
            print(f"âŒ é‡Šæ”¾è¿æ¥å›æ± å¤±è´¥: {e}")


# === æ–°å¢ï¼šåˆå§‹åŒ–æ•°æ®åº“è¡¨ ===
def init_db_tables():
    """åˆ›å»ºå¿…è¦çš„æ•°æ®åº“è¡¨"""
    conn = get_db_connection()
    if not conn:
        return False

    try:
        with conn.cursor() as cursor:
            # åˆ›å»ºæ¯”èµ›ä¿¡æ¯è¡¨ï¼ˆæ·»åŠ start_time_beijingåˆ°å”¯ä¸€çº¦æŸï¼‰
            cursor.execute("""
            CREATE TABLE IF NOT EXISTS matches (
                id SERIAL PRIMARY KEY,
                match_name TEXT NOT NULL,
                league_name TEXT NOT NULL,
                home_team TEXT NOT NULL,
                away_team TEXT NOT NULL,
                start_time_beijing TEXT NOT NULL,  -- æ–°å¢éç©ºçº¦æŸ
                time_until_start TEXT,
                result_value NUMERIC(10, 2) DEFAULT NULL,  -- æ–°å¢å­—æ®µ
                total_result NUMERIC(10, 2) DEFAULT NULL,  -- æ–°å¢ï¼šå¤§å°çƒç›˜æŒ‡æ•°ç»“æœ
                CONSTRAINT unique_match_time UNIQUE (match_name, start_time_beijing)  -- æ˜¾å¼å‘½åçº¦æŸ
            )
            """)

            # åˆ›å»ºèµ”ç‡å˜åŒ–è®°å½•è¡¨ - è®©åˆ†ç›˜
            cursor.execute("""
            CREATE TABLE IF NOT EXISTS spread_odds (
                id SERIAL PRIMARY KEY,
                match_id INTEGER NOT NULL,
                source INTEGER NOT NULL,
                spread_value TEXT NOT NULL,
                side TEXT NOT NULL,  -- 'home' æˆ– 'away'
                odds_value NUMERIC(6,3),
                recorded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (match_id) REFERENCES matches (id)
            )
            """)

            # åˆ›å»ºèµ”ç‡å˜åŒ–è®°å½•è¡¨ - å¤§å°çƒ
            cursor.execute("""
            CREATE TABLE IF NOT EXISTS total_odds (
                id SERIAL PRIMARY KEY,
                match_id INTEGER NOT NULL,
                source INTEGER NOT NULL,
                total_value TEXT NOT NULL,
                side TEXT NOT NULL,  -- 'over' æˆ– 'under'
                odds_value NUMERIC(6,3),
                recorded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (match_id) REFERENCES matches (id)
            )
            """)

            # åˆ›å»ºç´¢å¼•ä»¥åŠ é€ŸæŸ¥è¯¢ï¼ˆåŒ…å«start_time_beijingï¼‰
            cursor.execute(
                "CREATE INDEX IF NOT EXISTS idx_matches_name_time ON matches (match_name, start_time_beijing)")
            cursor.execute(
                "CREATE INDEX IF NOT EXISTS idx_spread_odds ON spread_odds (match_id, source, spread_value, side, recorded_at)")
            cursor.execute(
                "CREATE INDEX IF NOT EXISTS idx_total_odds ON total_odds (match_id, source, total_value, side, recorded_at)")

            conn.commit()
            print("âœ… æ•°æ®åº“è¡¨åˆå§‹åŒ–æˆåŠŸ")
            return True
    except Exception as e:
        print(f"âŒ æ•°æ®åº“è¡¨åˆå§‹åŒ–å¤±è´¥: {e}")
        conn.rollback()
        return False
    finally:
        release_db_connection(conn)


# === æ–°å¢ï¼šå°†æ¯”èµ›ä¿¡æ¯å­˜å…¥æ•°æ®åº“ï¼ˆåŒ…å«start_time_beijingï¼‰ ===
def save_match_info(match_name: str, match_data: Dict) -> Optional[int]:
    """ä¿å­˜æ¯”èµ›åŸºæœ¬ä¿¡æ¯åˆ°æ•°æ®åº“ï¼Œè¿”å›match_idï¼ˆä½¿ç”¨match_name + start_time_beijingä½œä¸ºå”¯ä¸€æ ‡è¯†ï¼‰"""
    conn = get_db_connection()
    if not conn:
        return None

    try:
        with conn.cursor() as cursor:
            # æå–resultå€¼ï¼ˆä»calculate_is189çš„è¿”å›ç»“æœä¸­è·å–ï¼‰
            result_value = match_data.get("result", None)  # å…³é”®è¡Œï¼šè·å–è®¡ç®—ç»“æœ
            # æå–å¤§å°çƒç›˜æŒ‡æ•°ç»“æœï¼ˆæ–°å¢ï¼‰
            total_result = match_data.get("total_result", None)
            # æ’å…¥æˆ–æ›´æ–°æ¯”èµ›ä¿¡æ¯ï¼ˆåŸºäºmatch_nameå’Œstart_time_beijingï¼‰
            cursor.execute("""
            INSERT INTO matches (match_name, league_name, home_team, away_team, start_time_beijing, time_until_start, result_value, total_result)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
            ON CONFLICT (match_name, start_time_beijing) DO UPDATE
            SET league_name = EXCLUDED.league_name,
                home_team = EXCLUDED.home_team,
                away_team = EXCLUDED.away_team,
                time_until_start = EXCLUDED.time_until_start,
                result_value = EXCLUDED.result_value,  -- ä»…æ›´æ–°resultå­—æ®µ
                total_result = EXCLUDED.total_result
            RETURNING id
            """, (
                match_name,
                match_data["league_name"],
                match_data["home_team"],
                match_data["away_team"],
                match_data["start_time_beijing"],  # å¿…é¡»éç©º
                match_data["time_until_start"],
                result_value,
                total_result
            ))

            match_id = cursor.fetchone()[0]
            conn.commit()
            return match_id
    except Exception as e:
        print(f"âŒ ä¿å­˜æ¯”èµ›ä¿¡æ¯å¤±è´¥: {e}")
        conn.rollback()
        return None
    finally:
        release_db_connection(conn)


# === ä¿®æ”¹åçš„ä¿å­˜èµ”ç‡å˜åŒ–å‡½æ•° ===
def save_odds_changes(match_id: int, match_name: str, changes: List[Dict]):
    """ä¿å­˜èµ”ç‡å˜åŒ–åˆ°æ•°æ®åº“ï¼ˆæ˜ç¡®åŒºåˆ†æ–¹å‘ï¼Œé¿å…æ··æ·†ï¼‰"""
    if not changes:
        return

    conn = get_db_connection()
    if not conn:
        return

    try:
        with conn.cursor() as cursor:
            for change in changes:
                table = "spread_odds" if change["type"] == "spread" else "total_odds"
                field = "spread_value" if change["type"] == "spread" else "total_value"

                # ç¡®ä¿æ•°å€¼ç±»å‹æ­£ç¡®ï¼ˆå¤„ç†å¯èƒ½çš„å­—ç¬¦ä¸²æ ¼å¼ï¼‰
                value = change[f"{change['type']}_value"]
                try:
                    # å°è¯•è½¬æ¢ä¸ºæµ®ç‚¹æ•°ï¼ˆé€‚ç”¨äºç›˜å£å€¼å¦‚ "0.5" æˆ– "-0.75"ï¼‰
                    float(value)
                except ValueError:
                    # è‹¥æ— æ³•è½¬æ¢ï¼ˆå¦‚æ•°æ®æºè¿”å›å¼‚å¸¸å€¼ï¼‰ï¼Œè®°å½•åŸå§‹å€¼
                    pass

                cursor.execute(f"""
                INSERT INTO {table} (match_id, source, {field}, side, odds_value)
                VALUES (%s, %s, %s, %s, %s)
                """, (
                    match_id,
                    change["source"],
                    change[f"{change['type']}_value"],
                    change["side"],
                    change["new_value"]  # å…è®¸ None å€¼
                ))

            conn.commit()
            print(f"âœ… å·²ä¿å­˜ {match_name} çš„ {len(changes)} æ¡èµ”ç‡å˜åŒ–è®°å½•")
    except Exception as e:
        print(f"âŒ ä¿å­˜èµ”ç‡å˜åŒ–å¤±è´¥: {e}")
        conn.rollback()
    finally:
        release_db_connection(conn)


def batch_fetch_bindings(league_names: List[str]) -> Dict[str, List[Dict[str, Any]]]:
    """æ‰¹é‡è·å–å¤šä¸ªè”èµ›çš„bindingsæ•°æ®ï¼Œä½¿ç”¨æŠ•ç¥¨æœºåˆ¶é€‰æ‹©æœ€å¸¸è§çš„è”èµ›åç§°ï¼ˆæ–°å¢å‰ä¸‰å€™é€‰ï¼‰"""
    if not league_names:
        return {}
    conn = get_db_connection()
    if not conn:
        return {}
    league_bindings = defaultdict(list)

    try:
        with conn.cursor(cursor_factory=DictCursor) as cursor:
            # ä¿®æ”¹æŸ¥è¯¢æ¡ä»¶ï¼šåŸºäºsource2_leagueè€Œésource3_league
            query = """
            SELECT * FROM bindings 
            WHERE source2_league = ANY(%s)
            """
            cursor.execute(query, (list(league_names),))

            # ç»Ÿè®¡å„è”èµ›åœ¨source1ä¸­çš„åç§°é¢‘ç‡
            league_votes = defaultdict(lambda: {"source1": defaultdict(int)})
            all_bindings = defaultdict(list)

            # æ”¶é›†æ‰€æœ‰ç»‘å®šè®°å½•å¹¶ç»Ÿè®¡åç§°é¢‘ç‡
            for binding in cursor.fetchall():
                s2_league = binding['source2_league']
                all_bindings[s2_league].append(binding)
                league_votes[s2_league]["source1"][binding['source1_league']] += 1

            # ä¸ºæ¯ä¸ªè”èµ›ç”Ÿæˆæœ€ç»ˆçš„ç»‘å®šè®°å½•ï¼ˆæ–°å¢ï¼šä¿å­˜å‰ä¸‰å€™é€‰ï¼‰
            for s2_league, votes in league_votes.items():
                # ç¡®å®šsource1çš„å€™é€‰åç§°ï¼ˆç¥¨æ•°å‰ä¸‰ï¼Œé™åºæ’åˆ—ï¼‰
                s1_votes = votes["source1"]
                s1_candidates = sorted(s1_votes.items(), key=lambda x: (-x[1], x[0]))[:3]  # å–ç¥¨æ•°å‰ä¸‰
                s1_candidate_names = [name for name, _ in s1_candidates] if s1_candidates else []

                # å¤„ç†æ— æŠ•ç¥¨æ•°æ®çš„æƒ…å†µï¼ˆç”¨åŸå§‹ç»‘å®šåç§°è¡¥å…¨ï¼‰
                if not s1_candidate_names and all_bindings[s2_league]:
                    s1_candidate_names = [binding['source1_league'] for binding in all_bindings[s2_league]][:3]

                # æ„å»ºæœ€ç»ˆçš„ç»‘å®šè®°å½•ï¼ˆæ–°å¢source1_candidateså­—æ®µï¼‰
                for binding in all_bindings[s2_league]:
                    league_bindings[s2_league].append({
                        "source1_league": s1_candidate_names[0] if s1_candidate_names else binding['source1_league'],
                        "source1_candidates": s1_candidate_names,  # æ–°å¢ï¼šå‰ä¸‰å€™é€‰åˆ—è¡¨
                        "source1_home_team": binding['source1_home_team'],
                        "source1_away_team": binding['source1_away_team'],
                        "source2_league": s2_league,
                        "source2_home_team": binding['source2_home_team'],
                        "source2_away_team": binding['source2_away_team'],
                    })

    except Exception as e:
        print(f"âŒ æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {e}")
    finally:
        release_db_connection(conn)
    return league_bindings


def create_team_mapping_cache(bindings: List[Dict[str, Any]]) -> Dict[str, Dict[str, str]]:
    """åˆ›å»ºçƒé˜Ÿæ˜ å°„ç¼“å­˜ï¼Œä»…åŒ…å«source1å’Œsource2å‡æœ‰å€¼çš„è®°å½•"""
    mapping_cache = {}
    for binding in bindings:
        home_team = binding['source2_home_team']  # ç°åœ¨ä»¥source2ä¸ºåŸºå‡†
        away_team = binding['source2_away_team']  # ç°åœ¨ä»¥source2ä¸ºåŸºå‡†
        if binding['source1_home_team'] and binding['source2_home_team']:
            mapping_cache[home_team] = {
                "source1": binding['source1_home_team'],
                "source2": binding['source2_home_team']
            }
        if binding['source1_away_team'] and binding['source2_away_team']:
            mapping_cache[away_team] = {
                "source1": binding['source1_away_team'],
                "source2": binding['source2_away_team']
            }
    return mapping_cache


def create_api_index(api_data: List[Dict[str, Any]]) -> Dict[Tuple[str, str, str], Dict[str, Any]]:
    """åˆ›å»ºAPIæ•°æ®ç´¢å¼•ï¼ŒåŠ é€ŸæŸ¥æ‰¾"""
    return {(match["league_name"], match["home_team"], match["away_team"]): match for match in api_data}


async def process_league(league_name: str, matches: List[Dict[str, Any]],
                         all_api_indexes: Dict[int, Dict[Tuple[str, str, str], Dict[str, Any]]],
                         league_bindings: List[Dict[str, Any]]):
    """å¤„ç†å•ä¸ªè”èµ›çš„æ¯”èµ›ï¼Œå¿…é¡»åŒæ—¶åŒ¹é…source1å’Œsource2æ‰è§†ä¸ºæˆåŠŸï¼ˆæ–°å¢source1å€™é€‰åŒ¹é…ï¼‰"""
    if not league_bindings:
        print(f"âŒ è”èµ› {league_name} æ²¡æœ‰æ‰¾åˆ°ç»‘å®šæ•°æ®ï¼Œè·³è¿‡å¤„ç†")
        return []

    team_mapping_cache = create_team_mapping_cache(league_bindings)
    if not team_mapping_cache:
        print(f"âŒ è”èµ› {league_name} æ²¡æœ‰æœ‰æ•ˆçš„çƒé˜Ÿæ˜ å°„æ•°æ®ï¼Œè·³è¿‡å¤„ç†")
        return []

    league_info = {
        "source1": league_bindings[0]["source1_league"],
        "source2": league_name  # ç°åœ¨ä»¥source2ä¸ºåŸºå‡†
    }
    # è·å–source1å‰ä¸‰å€™é€‰è”èµ›åç§°ï¼ˆæ–°å¢ï¼‰
    s1_candidates = league_bindings[0].get("source1_candidates", [league_info["source1"]])[:3]

    results = []
    required_sources = {1, 2}  # ä»…éœ€åŒ¹é…source1å’Œsource2

    # ç»Ÿè®¡ä¿¡æ¯
    total_matches = len(matches)
    matched_count = 0
    unmatched_count = 0
    missing_team_mapping = 0

    print(f"ğŸ“Š å¼€å§‹å¤„ç†è”èµ›: {league_name}, å…±æœ‰ {total_matches} åœºæ¯”èµ›")
    print(f"ğŸ” source1å€™é€‰è”èµ›åç§°: {s1_candidates}")  # æ–°å¢ï¼šæ‰“å°å€™é€‰åˆ—è¡¨

    for match in matches:
        home_team = match["home_team"]
        away_team = match["away_team"]

        # æ£€æŸ¥çƒé˜Ÿæ˜¯å¦æœ‰æ˜ å°„å…³ç³»
        if home_team not in team_mapping_cache or away_team not in team_mapping_cache:
            print(f"âŒ æ¯”èµ› {home_team} vs {away_team} æ— æ³•åŒ¹é…ï¼šçƒé˜Ÿæ˜ å°„ç¼ºå¤±")
            missing_team_mapping += 1
            unmatched_count += 1
            continue

        home_mapping = team_mapping_cache[home_team]
        away_mapping = team_mapping_cache[away_team]

        # æ‰“å°å„æºçƒé˜Ÿåç§°ç”¨äºå¯¹æ¯”
        print(f"ğŸ” æ¯”èµ›: {home_team} vs {away_team}")
        print(f"  source2çƒé˜Ÿ: {home_team} vs {away_team}")
        print(f"  source1æ˜ å°„: {home_mapping['source1']} vs {away_mapping['source1']}")

        # å°è¯•åœ¨ä¸¤ä¸ªæ•°æ®æºä¸­æŸ¥æ‰¾åŒ¹é…çš„æ¯”èµ›
        matched_apis = {}
        missing_sources = set(required_sources)

        # å…ˆå¤„ç†source2ï¼ˆé€»è¾‘ä¸å˜ï¼‰
        if 2 in required_sources:
            db_key = (league_info["source2"], home_team, away_team)
            source_league = league_info["source2"]
            source_home = home_team
            source_away = away_team
            api_index = all_api_indexes.get(2, {})
            print(f"  ğŸ” source2 åŒ¹é…é”®: ({source_league}, {source_home}, {source_away})")

            if db_key in api_index:
                matched_apis[2] = api_index[db_key]
                missing_sources.discard(2)
                print(f"  âœ… source2 åŒ¹é…æˆåŠŸ")
            else:
                print(f"  âš ï¸ source2 æœªæ‰¾åˆ°åŒ¹é…é”®")

        # å¤„ç†source1ï¼ˆæ–°å¢ï¼šéå†å‰ä¸‰å€™é€‰ï¼‰
        if 1 in required_sources and 2 in matched_apis:  # source2åŒ¹é…æˆåŠŸåæ‰å°è¯•source1
            api_index = all_api_indexes.get(1, {})
            home_key = home_mapping["source1"]
            away_key = away_mapping["source1"]
            match_found = False

            # ä¾æ¬¡å°è¯•å‰ä¸‰å€™é€‰
            for i, league_key in enumerate(s1_candidates):
                if not league_key:
                    continue
                db_key = (league_key, home_key, away_key)
                print(f"  ğŸ” source1 å€™é€‰{i+1}åŒ¹é…é”®: ({league_key}, {home_key}, {away_key})")

                if db_key in api_index:
                    matched_apis[1] = api_index[db_key]
                    missing_sources.discard(1)
                    print(f"  âœ… source1 å€™é€‰{i+1}åŒ¹é…æˆåŠŸ")
                    match_found = True
                    break
                else:
                    print(f"  âš ï¸ source1 å€™é€‰{i+1}æœªæ‰¾åˆ°åŒ¹é…é”®")

            if not match_found:
                print(f"  âŒ source1 å‰ä¸‰å€™é€‰å‡åŒ¹é…å¤±è´¥")

        # åªæœ‰å½“ä¸¤ä¸ªæ•°æ®æºéƒ½åŒ¹é…æˆåŠŸæ—¶æ‰æ·»åŠ åˆ°ç»“æœä¸­
        if not missing_sources:
            # ä¸ºä¿æŒæ•°æ®ç»“æ„ä¸€è‡´ï¼Œæ·»åŠ ç©ºçš„source3æ•°æ®
            matched_apis[3] = {
                "league_name": league_info["source2"],  # ä½¿ç”¨source2çš„è”èµ›åä½œä¸ºé»˜è®¤å€¼
                "home_team": home_team,
                "away_team": away_team,
                "odds": {"spreads": {}, "totals": {}}  # ç©ºèµ”ç‡æ•°æ®
            }

            results.append((match, {"home": home_mapping, "away": away_mapping, "league": league_info}, matched_apis))
            matched_count += 1
            print(f"âœ… æ¯”èµ› {home_team} vs {away_team} ä¸¤æºåŒ¹é…æˆåŠŸ")
        else:
            missing_msg = ", ".join([f"source{src}" for src in missing_sources])
            print(f"âŒ æ¯”èµ› {home_team} vs {away_team} åŒ¹é…å¤±è´¥ï¼šç¼ºå°‘æ•°æ®æº {missing_msg}")
            unmatched_count += 1

    # æ‰“å°è”èµ›å¤„ç†ç»Ÿè®¡ä¿¡æ¯
    print(f"ğŸ“Š è”èµ› {league_name} å¤„ç†å®Œæˆ:")
    print(f"  - æ€»æ¯”èµ›æ•°: {total_matches}")
    print(f"  - æˆåŠŸåŒ¹é…: {matched_count} ({matched_count / total_matches * 100:.2f}%)")
    print(f"  - åŒ¹é…å¤±è´¥: {unmatched_count} ({unmatched_count / total_matches * 100:.2f}%)")
    print(f"  - å› çƒé˜Ÿæ˜ å°„ç¼ºå¤±å¤±è´¥: {missing_team_mapping}")

    return results


def calculate_common_odds(source1_odds, source2_odds, source3_odds):
    """
    ä¿®å¤ï¼šåªè®¡ç®—source1å’Œsource2çš„èµ”ç‡äº¤é›†ï¼ˆå¿½ç•¥source3ï¼Œå› ä¸ºå®ƒå·²ä¸ºç©ºï¼‰
    è¿”å›æ ¼å¼: {
        "spreads": {ç›˜å£å€¼: {"home": æ˜¯å¦å­˜åœ¨, "away": æ˜¯å¦å­˜åœ¨}},
        "totals": {ç›˜å£å€¼: {"over": æ˜¯å¦å­˜åœ¨, "under": æ˜¯å¦å­˜åœ¨}}
    }
    """
    common_spreads = {}
    common_totals = {}

    # å¤„ç†è®©åˆ†ç›˜äº¤é›†ï¼ˆä»…è€ƒè™‘source1å’Œsource2ï¼‰
    all_spreads = set()
    all_spreads.update(source1_odds.get('spreads', {}).keys())
    all_spreads.update(source2_odds.get('spreads', {}).keys())

    for spread in all_spreads:
        # åªæ£€æŸ¥source1å’Œsource2æ˜¯å¦æœ‰è¯¥ç›˜å£
        has_source1 = spread in source1_odds.get('spreads', {})
        has_source2 = spread in source2_odds.get('spreads', {})

        if has_source1 and has_source2:
            # æ£€æŸ¥ä¸»å®¢é˜Ÿæ–¹å‘æ˜¯å¦åœ¨ä¸¤ä¸ªæ•°æ®æºéƒ½å­˜åœ¨
            home_in_all = (
                    'home' in source1_odds['spreads'][spread] and
                    'home' in source2_odds['spreads'][spread]
            )
            away_in_all = (
                    'away' in source1_odds['spreads'][spread] and
                    'away' in source2_odds['spreads'][spread]
            )

            if home_in_all or away_in_all:
                common_spreads[spread] = {
                    'home': home_in_all,
                    'away': away_in_all
                }

    # å¤„ç†å¤§å°çƒäº¤é›†ï¼ˆä»…è€ƒè™‘source1å’Œsource2ï¼‰
    all_totals = set()
    all_totals.update(source1_odds.get('totals', {}).keys())
    all_totals.update(source2_odds.get('totals', {}).keys())

    for total in all_totals:
        # åªæ£€æŸ¥source1å’Œsource2æ˜¯å¦æœ‰è¯¥ç›˜å£
        has_source1 = total in source1_odds.get('totals', {})
        has_source2 = total in source2_odds.get('totals', {})

        if has_source1 and has_source2:
            over_in_all = (
                    'over' in source1_odds['totals'][total] and
                    'over' in source2_odds['totals'][total]
            )
            under_in_all = (
                    'under' in source1_odds['totals'][total] and
                    'under' in source2_odds['totals'][total]
            )

            if over_in_all or under_in_all:
                common_totals[total] = {
                    'over': over_in_all,
                    'under': under_in_all
                }

    return {
        'spreads': common_spreads,
        'totals': common_totals
    }


def calculate_odds_max(matches_data: Dict) -> Dict:
    """è®¡ç®—æ¯åœºæ¯”èµ›æ¯ä¸ªç›˜å£çš„æœ€å¤§èµ”ç‡å¹¶æ·»åŠ åˆ°æ•°æ®ä¸­ï¼Œå¤„ç†å­—ç¬¦ä¸²èµ”ç‡å’Œç‰¹æ®Šæ­£è´Ÿæ•°é€»è¾‘"""
    for match_key, match_data in matches_data.items():
        # åˆå§‹åŒ–maxå­—æ®µ
        match_data['max'] = {
            'spreads': {},
            'totals': {}
        }

        # è·å–æ‰€æœ‰æ•°æ®æºçš„èµ”ç‡
        source_odds = [source.get('odds', {}) for source in match_data.get('sources', [])]

        # æ”¶é›†æ‰€æœ‰å­˜åœ¨çš„ç›˜å£å€¼
        all_spread_values = set()
        for source_odd in source_odds:
            all_spread_values.update(source_odd.get('spreads', {}).keys())

        all_total_values = set()
        for source_odd in source_odds:
            all_total_values.update(source_odd.get('totals', {}).keys())

        # === è®¡ç®—è®©åˆ†ç›˜çš„æœ€å¤§å€¼ï¼ˆå¤„ç†å­—ç¬¦ä¸²ã€ç©ºå€¼å’Œæ­£è´Ÿæ•°é€»è¾‘ï¼‰ ===
        for spread_value in all_spread_values:
            match_data['max']['spreads'][spread_value] = {
                'home': None,
                'away': None
            }
            for direction in ['home', 'away']:
                values = []
                for source_odd in source_odds:
                    if spread_value in source_odd.get('spreads', {}) and direction in source_odd['spreads'][
                        spread_value]:
                        value = source_odd['spreads'][spread_value][direction]
                        # å¤„ç†å­—ç¬¦ä¸²å€¼
                        if isinstance(value, str):
                            try:
                                value = float(value)
                            except ValueError:
                                value = None  # æ— æ³•è½¬æ¢åˆ™è®¾ä¸º None
                        # ä»…æ·»åŠ æœ‰æ•ˆæ•°å€¼
                        if isinstance(value, (int, float)) and value is not None:
                            values.append(value)

                # === æ–°å¢ï¼šå¤„ç†æ­£è´Ÿæ•°é€»è¾‘ ===
                if values:
                    has_positive = any(v > 0 for v in values)
                    has_negative = any(v < 0 for v in values)

                    if has_positive and has_negative:  # åŒæ—¶å­˜åœ¨æ­£è´Ÿæ•°
                        negatives = [v for v in values if v < 0]
                        if negatives:  # å¦‚æœæœ‰è´Ÿæ•°ï¼Œåˆ™å–è´Ÿæ•°çš„æœ€å¤§å€¼
                            match_data['max']['spreads'][spread_value][direction] = max(negatives)
                        else:  # ç†è®ºä¸Šä¸ä¼šå‡ºç°ï¼ˆå·²æœ‰è´Ÿæ•°åˆ¤æ–­ï¼‰
                            match_data['max']['spreads'][spread_value][direction] = max(values)
                    else:  # å…¨æ­£æ•°æˆ–å…¨è´Ÿæ•°ï¼Œç›´æ¥å–æœ€å¤§å€¼
                        match_data['max']['spreads'][spread_value][direction] = max(values)

        # === è®¡ç®—å¤§å°çƒçš„æœ€å¤§å€¼ï¼ˆåŒç†ï¼‰ ===
        for total_value in all_total_values:
            match_data['max']['totals'][total_value] = {
                'over': None,
                'under': None
            }
            for direction in ['over', 'under']:
                values = []
                for source_odd in source_odds:
                    if total_value in source_odd.get('totals', {}) and direction in source_odd['totals'][total_value]:
                        value = source_odd['totals'][total_value][direction]
                        # å¤„ç†å­—ç¬¦ä¸²å€¼
                        if isinstance(value, str):
                            try:
                                value = float(value)
                            except ValueError:
                                value = None  # æ— æ³•è½¬æ¢åˆ™è®¾ä¸º None
                        # ä»…æ·»åŠ æœ‰æ•ˆæ•°å€¼
                        if isinstance(value, (int, float)) and value is not None:
                            values.append(value)

                # === å¤„ç†æ­£è´Ÿæ•°é€»è¾‘ ===
                if values:
                    has_positive = any(v > 0 for v in values)
                    has_negative = any(v < 0 for v in values)

                    if has_positive and has_negative:  # åŒæ—¶å­˜åœ¨æ­£è´Ÿæ•°
                        negatives = [v for v in values if v < 0]
                        if negatives:  # å¦‚æœæœ‰è´Ÿæ•°ï¼Œåˆ™å–è´Ÿæ•°çš„æœ€å¤§å€¼
                            match_data['max']['totals'][total_value][direction] = max(negatives)
                        else:  # ç†è®ºä¸Šä¸ä¼šå‡ºç°ï¼ˆå·²æœ‰è´Ÿæ•°åˆ¤æ–­ï¼‰
                            match_data['max']['totals'][total_value][direction] = max(values)
                    else:  # å…¨æ­£æ•°æˆ–å…¨è´Ÿæ•°ï¼Œç›´æ¥å–æœ€å¤§å€¼
                        match_data['max']['totals'][total_value][direction] = max(values)

        # === è¿‡æ»¤æ‰å€¼ä¸º None çš„æ–¹å‘ ===
        # å¤„ç†è®©åˆ†ç›˜
        spreads = match_data['max']['spreads']
        for spread_value in list(spreads.keys()):
            directions = spreads[spread_value]
            valid_directions = {k: v for k, v in directions.items() if v is not None}
            if valid_directions:
                spreads[spread_value] = valid_directions
            else:
                del spreads[spread_value]

        # å¤„ç†å¤§å°çƒï¼ˆåŒç†ï¼‰
        totals = match_data['max']['totals']
        for total_value in list(totals.keys()):
            directions = totals[total_value]
            valid_directions = {k: v for k, v in directions.items() if v is not None}
            if valid_directions:
                totals[total_value] = valid_directions
            else:
                del totals[total_value]

    return matches_data


def calculate_is189(match_data: Dict) -> Dict:
    """
    è®¡ç®—189æŒ‡æ•°ï¼ŒåŒ…å«0ç›˜å£ï¼ˆå¹³æ‰‹ç›˜ï¼‰çš„ç‰¹æ®Šå¤„ç†
    ä¿®å¤1å’Œ-1ç›˜å£åŒ¹é…é—®é¢˜ï¼Œç¡®ä¿æµ®ç‚¹æ•°è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ—¶ä¿ç•™æ•´æ•°æ ¼å¼
    æ”¹åŠ¨ï¼šè®¡ç®—æ‰€æœ‰æœ‰æ•ˆç›˜å£ï¼Œè¿”å›æœ€å¤§çš„189æŒ‡æ•°å€¼
    """
    source2 = next((s for s in match_data.get("sources", []) if s["source"] == 2), None)
    if not source2 or not source2.get("odds") or not source2["odds"].get("spreads"):
        return {
            "is189": False,
            "result": None,
            "calculation_detail": "æ•°æ®æº2ç¼ºå¤±æˆ–æ— è®©åˆ†ç›˜æ•°æ®",
            "all_calculations": []  # æ–°å¢ï¼šå­˜å‚¨æ‰€æœ‰è®¡ç®—ç»“æœ
        }

    spreads = source2["odds"]["spreads"]
    max_result = None
    best_direction = None
    all_calculations = []  # æ–°å¢ï¼šå­˜å‚¨æ‰€æœ‰è®¡ç®—ç»“æœ

    for spread_str, directions in spreads.items():
        try:
            home_spread = float(spread_str)
            opposite_spread = -home_spread

            # å…³é”®ä¿®æ”¹ï¼šå°†æµ®ç‚¹æ•°è½¬æ¢ä¸ºæ•´æ•°æ ¼å¼å­—ç¬¦ä¸²ï¼ˆå¦‚-1.0è½¬ä¸º-1ï¼‰
            if home_spread.is_integer():
                spread_str_clean = f"{int(home_spread)}"
            else:
                spread_str_clean = f"{home_spread}"

            if opposite_spread.is_integer():
                opposite_spread_str = f"{int(opposite_spread)}"
            else:
                opposite_spread_str = f"{opposite_spread}"

            # å¤„ç†0ç›˜å£
            if home_spread == 0:
                opposite_spread_str = spread_str_clean  # 0çš„ç›¸åæ•°è¿˜æ˜¯0

            if opposite_spread_str not in spreads:
                continue  # ç¡®ä¿å®¢é˜Ÿç›˜å£å­˜åœ¨

            # æå–èµ”ç‡
            home_odd_str = directions.get("home", "0")
            away_odd_str = spreads[opposite_spread_str].get("away", "0")

            # 0ç›˜å£å…¼å®¹å¤„ç†
            if home_spread == 0 and not away_odd_str:
                away_odd_str = directions.get("away", "0")

            home_odd = float(home_odd_str)
            away_odd = float(away_odd_str)

            if home_odd == 0 or away_odd == 0:
                continue

            h_abs = abs(home_odd)
            a_abs = abs(away_odd)
            abs_diff = max(h_abs, a_abs) - min(h_abs, a_abs)

            calculation_steps = [
                f"ç›˜å£: {spread_str_clean}ï¼ˆä¸»é˜Ÿï¼‰ vs {opposite_spread_str}ï¼ˆå®¢é˜Ÿï¼‰",
                f"ä¸»é˜Ÿèµ”ç‡: {home_odd_str}",
                f"å®¢é˜Ÿèµ”ç‡: {away_odd_str}",
                f"ç»å¯¹å€¼å·®: {abs_diff:.3f}",
            ]

            if home_odd * away_odd > 0:
                current_result = (home_odd + away_odd) * 100
            else:
                current_result = (2 - abs_diff) * 100

            current_result_rounded = round(current_result, 2)

            # æ–°å¢ï¼šè®°å½•æ‰€æœ‰è®¡ç®—ç»“æœ
            calculation_record = {
                "spread": spread_str_clean,
                "opposite_spread": opposite_spread_str,
                "home_odd": home_odd,
                "away_odd": away_odd,
                "result": current_result_rounded,
                "steps": calculation_steps
            }
            all_calculations.append(calculation_record)

            # æ›´æ–°æœ€å¤§å€¼
            if max_result is None or current_result_rounded > max_result:
                max_result = current_result_rounded
                best_direction = calculation_steps

        except (ValueError, TypeError):
            continue

    if max_result is not None:
        return {
            "is189": max_result == 189.0,
            "result": max_result,
            "calculation_detail": "\n".join(best_direction) if best_direction else "",
            "all_calculations": all_calculations  # æ–°å¢ï¼šè¿”å›æ‰€æœ‰è®¡ç®—ç»“æœ
        }
    else:
        return {
            "is189": False,
            "result": None,
            "calculation_detail": "æœªæ‰¾åˆ°æœ‰æ•ˆçš„ç›˜å£å¯¹ï¼ˆå«0ç›˜å£ï¼‰",
            "all_calculations": []
        }


def calculate_total_189(match_data: Dict) -> Dict:
    """
    å¤§å°çƒç›˜189æŒ‡æ•°è®¡ç®—ï¼ˆç›˜å£åŒ¹é…æ–¹å¼ç‹¬ç«‹ï¼Œè®¡ç®—é€»è¾‘ä¸è®©åˆ†ç›˜ä¸€è‡´ï¼‰
    ç›´æ¥ä½¿ç”¨åŒä¸€ç›˜å£çš„å¤§çƒå’Œå°çƒèµ”ç‡ï¼Œæ ¸å¿ƒè®¡ç®—é€»è¾‘ä¸è®©åˆ†ç›˜å®Œå…¨ç›¸åŒ
    æ”¹åŠ¨ï¼šè®¡ç®—æ‰€æœ‰æœ‰æ•ˆç›˜å£ï¼Œè¿”å›æœ€å¤§çš„189æŒ‡æ•°å€¼
    """
    source2 = next((s for s in match_data.get("sources", []) if s["source"] == 2), None)
    if not source2 or not source2.get("odds") or not source2["odds"].get("totals"):
        return {
            "is_total_189": False,
            "total_result": None,
            "total_calculation_detail": "æ•°æ®æº2ç¼ºå¤±æˆ–æ— å¤§å°çƒç›˜æ•°æ®",
            "all_total_calculations": []  # æ–°å¢ï¼šå­˜å‚¨æ‰€æœ‰è®¡ç®—ç»“æœ
        }

    totals = source2["odds"]["totals"]
    max_result = None
    best_direction = None
    all_calculations = []  # æ–°å¢ï¼šå­˜å‚¨æ‰€æœ‰è®¡ç®—ç»“æœ

    for total_str, directions in totals.items():
        try:
            # å¤§å°çƒç›˜å£æ— éœ€è®¡ç®—ç›¸åæ•°ï¼Œç›´æ¥ä½¿ç”¨å½“å‰ç›˜å£
            total_value = float(total_str)

            # è§„èŒƒåŒ–ç›˜å£å­—ç¬¦ä¸²æ ¼å¼ï¼ˆä¸è®©åˆ†ç›˜ä¸€è‡´çš„å¤„ç†æ–¹å¼ï¼‰
            if total_value.is_integer():
                total_str_clean = f"{int(total_value)}"
            else:
                total_str_clean = f"{total_value}"

            # ç¡®ä¿ç›˜å£åŒæ—¶åŒ…å«å¤§çƒå’Œå°çƒèµ”ç‡
            if "over" not in directions or "under" not in directions:
                continue

            # æå–èµ”ç‡ï¼ˆåŒä¸€ç›˜å£çš„å¤§çƒå’Œå°çƒèµ”ç‡ï¼‰
            over_odd_str = directions.get("over", "0")
            under_odd_str = directions.get("under", "0")

            over_odd = float(over_odd_str)
            under_odd = float(under_odd_str)

            if over_odd == 0 or under_odd == 0:
                continue

            # è®¡ç®—èµ”ç‡ç»å¯¹å€¼ï¼ˆä¸è®©åˆ†ç›˜ä¸€è‡´çš„å¤„ç†é€»è¾‘ï¼‰
            h_abs = abs(over_odd)
            a_abs = abs(under_odd)
            abs_diff = max(h_abs, a_abs) - min(h_abs, a_abs)

            calculation_steps = [
                f"å¤§å°çƒç›˜: {total_str_clean}",
                f"å¤§çƒèµ”ç‡: {over_odd_str}",
                f"å°çƒèµ”ç‡: {under_odd_str}",
                f"ç»å¯¹å€¼å·®: {abs_diff:.3f}",
            ]

            # æ ¸å¿ƒè®¡ç®—é€»è¾‘ä¸è®©åˆ†ç›˜å®Œå…¨ä¸€è‡´
            if over_odd * under_odd > 0:
                current_result = (over_odd + under_odd) * 100
            else:
                current_result = (2 - abs_diff) * 100

            current_result_rounded = round(current_result, 2)

            # æ–°å¢ï¼šè®°å½•æ‰€æœ‰è®¡ç®—ç»“æœ
            calculation_record = {
                "total": total_str_clean,
                "over_odd": over_odd,
                "under_odd": under_odd,
                "result": current_result_rounded,
                "steps": calculation_steps
            }
            all_calculations.append(calculation_record)

            # æ›´æ–°æœ€å¤§å€¼
            if max_result is None or current_result_rounded > max_result:
                max_result = current_result_rounded
                best_direction = calculation_steps

        except (ValueError, TypeError):
            continue

    if max_result is not None:
        return {
            "is_total_189": max_result == 189.0,
            "total_result": max_result,
            "total_calculation_detail": "\n".join(best_direction) if best_direction else "",
            "all_total_calculations": all_calculations  # æ–°å¢ï¼šè¿”å›æ‰€æœ‰è®¡ç®—ç»“æœ
        }
    else:
        return {
            "is_total_189": False,
            "total_result": None,
            "total_calculation_detail": "æœªæ‰¾åˆ°åŒ…å«å¤§çƒå’Œå°çƒèµ”ç‡çš„æœ‰æ•ˆç›˜å£",
            "all_total_calculations": []
        }

# === æ ¸å¿ƒæ•°æ®å¤„ç† ===
@timed
async def process_api_data(results: List[Dict[str, Any]]):
    """å¤„ç†APIæ•°æ®å¹¶ç”Ÿæˆæœ€ç»ˆæ¯”èµ›æ•°æ®ï¼ˆä½¿ç”¨å”¯ä¸€é”®ï¼šmatch_name + start_time_beijingï¼‰"""
    print("============================================")
    print("å¼€å§‹å¤„ç†APIæ•°æ®...")

    all_api_data = {}
    all_api_indexes = {}

    for i, url in enumerate(API_URLS, 1):  # ç°åœ¨åªæœ‰ä¸¤ä¸ªAPI
        result = next((r for r in results if r["url"] == url), {"status": "error"})
        if result["status"] == "success":
            all_api_data[i] = result["data"]
            all_api_indexes[i] = create_api_index(result["data"])
            print(f"âœ… ä»source{i}è·å–äº† {len(result['data'])} åœºæ¯”èµ›æ•°æ®")
        else:
            all_api_data[i] = []
            all_api_indexes[i] = {}
            print(f"âŒ ä»source{i}è·å–æ•°æ®å¤±è´¥: {result.get('error_message', 'æœªçŸ¥é”™è¯¯')}")

    # ç°åœ¨ä»¥source2ä¸ºåŸºå‡†æ•°æ®æº
    source2_result = next((r for r in results if r["url"] == API_URLS[1]), None)
    if not source2_result or source2_result["status"] != "success":
        print("âŒ æœªè·å–åˆ°source2çš„æ•°æ®ï¼Œæ— æ³•ç»§ç»­å¤„ç†")
        return None, 0, 0  # æ–°å¢ï¼šè¿”å›é»˜è®¤å€¼é¿å…åç»­é”™è¯¯

    source2_data = source2_result["data"]
    total_matches_source2 = len(source2_data)
    print(f"ğŸ“Š source2å…±æœ‰ {total_matches_source2} åœºæ¯”èµ›")

    league_groups = defaultdict(list)
    for match in source2_data:
        league_groups[match["league_name"]].append(match)

    print(f"ğŸ” å‘ç° {len(league_groups)} ä¸ªä¸åŒçš„è”èµ›")

    league_bindings_map = batch_fetch_bindings(list(league_groups.keys()))

    # ç»Ÿè®¡è”èµ›ç»‘å®šæƒ…å†µ
    total_leagues = len(league_groups)
    leagues_with_bindings = sum(
        1 for league in league_groups if league in league_bindings_map and league_bindings_map[league])
    print(f"ğŸ“Š è”èµ›ç»‘å®šæƒ…å†µ: {leagues_with_bindings}/{total_leagues} ä¸ªè”èµ›æœ‰ç»‘å®šæ•°æ®")

    tasks = []
    for league_name, matches in league_groups.items():
        tasks.append(process_league(
            league_name,
            matches,
            all_api_indexes,
            league_bindings_map.get(league_name, [])
        ))

    league_results = await asyncio.gather(*tasks)

    all_matched_matches = [match for league_result in league_results for match in league_result]
    total_matched = len(all_matched_matches)  # è¿™å°±æ˜¯ä¸¤æºåŒ¹é…æˆåŠŸæ•°

    print(f"============================================")
    print(f"ğŸ“Š åŒ¹é…ç»“æœæ±‡æ€»:")
    print(f"  - source2æ€»æ¯”èµ›æ•°: {total_matches_source2}")
    print(f"  - ä¸¤æºåŒ¹é…æˆåŠŸ: {total_matched} ({total_matched / total_matches_source2 * 100:.2f}%)")
    print(f"  - åŒ¹é…å¤±è´¥: {total_matches_source2 - total_matched} ({(total_matches_source2 - total_matched) / total_matches_source2 * 100:.2f}%)")
    print(f"============================================")

    # ç”¨äºå­˜å‚¨æ‰€æœ‰æ¯”èµ›çš„æ•°æ®ï¼ˆä½¿ç”¨å”¯ä¸€é”®ï¼šmatch_name + start_time_beijingï¼‰
    all_matches_data = {}

    for match_tuple in all_matched_matches:
        match, team_mapping, matched_apis = match_tuple

        # æå–ä¸‰ä¸ªæ•°æ®æºçš„èµ”ç‡æ•°æ®
        source1_odds = matched_apis.get(1, {}).get('odds', {'spreads': {}, 'totals': {}})
        source2_odds = matched_apis.get(2, {}).get('odds', {'spreads': {}, 'totals': {}})
        source3_odds = matched_apis.get(3, {}).get('odds', {'spreads': {}, 'totals': {}})

        # ä½¿ç”¨æ–°çš„äº¤é›†è®¡ç®—æ–¹æ³•
        common_odds = calculate_common_odds(source1_odds, source2_odds, source3_odds)

        # ä½¿ç”¨æ•°æ®æº2çš„åç§°ä½œä¸ºæ¯”èµ›åç§°
        match_name = f"{team_mapping['league']['source2']} - {team_mapping['home']['source2']} vs {team_mapping['away']['source2']}"

        # ========== å…³é”®ä¿®æ”¹ ==========
        # ç›´æ¥ä»matched_apisè·å–å·²åŒ¹é…çš„source1/source2æ•°æ®ï¼ˆæ— éœ€é‡æ–°éå†ï¼‰
        source1_raw_match = matched_apis.get(1, {})
        source2_raw_match = matched_apis.get(2, {})
        # æ–°å¢ï¼šä»SOURCE1æå–event_idå’Œline_id
        event_id = source1_raw_match.get('event_id', '')
        line_id = source1_raw_match.get('line_id', '')
        league_id = source1_raw_match.get('league_id', '')

        # æå–start_time_beijingï¼ˆä¼˜å…ˆçº§ï¼šsource1 â†’ source2 â†’ å½“å‰æ—¶é—´ï¼‰
        start_time_beijing = source1_raw_match.get('start_time_beijing', '')
        if not start_time_beijing:
            start_time_beijing = source2_raw_match.get('start_time_beijing', '')
        if not start_time_beijing:
            start_time_beijing = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            print(f"âš ï¸ æ¯”èµ› {match_name} çš„start_time_beijingä¸ºç©ºï¼Œå·²ç”¨å½“å‰æ—¶é—´å…œåº•ï¼š{start_time_beijing}")
        # ========== å…³é”®ä¿®æ”¹ç»“æŸ ==========

        time_until_start = source1_raw_match.get('time_until_start', '') or source2_raw_match.get('time_until_start',
                                                                                                  '')

        # æ„å»ºæ•°æ®æºç»“æ„
        source_data = []
        for source_index in sorted(matched_apis.keys()):
            api_match = matched_apis[source_index]
            if source_index == 3:
                home_team = api_match['home_team']
                away_team = api_match['away_team']
                league_name = api_match['league_name']
            else:
                source_key = f"source{source_index}"
                home_team = team_mapping["home"][source_key]
                away_team = team_mapping["away"][source_key]
                league_name = team_mapping["league"][source_key]

            source_entry = {
                "source": source_index,
                "league": league_name,
                "home_team": home_team,
                "away_team": away_team,
                "odds": {
                    "spreads": {},
                    "totals": {}
                }
            }

            # å¤„ç†è®©åˆ†ç›˜ï¼ˆä»…ä¿ç•™äº¤é›†éƒ¨åˆ†ï¼‰
            for spread, directions in common_odds['spreads'].items():
                if spread in api_match.get('odds', {}).get('spreads', {}):
                    filtered_spread = {}
                    spread_data = api_match['odds']['spreads'][spread]  # è·å–åŸå§‹ç›˜å£æ•°æ®

                    # ä»…åœ¨source1æ•°æ®ä¸­æ·»åŠ altLineIdå­—æ®µ
                    if source_index == 1 and 'altLineId' in spread_data:
                        filtered_spread['altLineId'] = spread_data['altLineId']

                    if directions['home'] and 'home' in spread_data:
                        filtered_spread['home'] = spread_data['home']
                    if directions['away'] and 'away' in spread_data:
                        filtered_spread['away'] = spread_data['away']
                    if filtered_spread:  # ç¡®ä¿æœ‰æ•°æ®æ‰æ·»åŠ 
                        source_entry['odds']['spreads'][spread] = filtered_spread

            # å¤„ç†å¤§å°çƒï¼ˆä»…ä¿ç•™äº¤é›†éƒ¨åˆ†ï¼‰
            for total, directions in common_odds['totals'].items():
                if total in api_match.get('odds', {}).get('totals', {}):
                    filtered_total = {}
                    total_data = api_match['odds']['totals'][total]  # è·å–åŸå§‹ç›˜å£æ•°æ®

                    # ä»…åœ¨source1æ•°æ®ä¸­æ·»åŠ altLineIdå­—æ®µ
                    if source_index == 1 and 'altLineId' in total_data:
                        filtered_total['altLineId'] = total_data['altLineId']

                    if directions['over'] and 'over' in total_data:
                        filtered_total['over'] = total_data['over']
                    if directions['under'] and 'under' in total_data:
                        filtered_total['under'] = total_data['under']
                    if filtered_total:
                        source_entry['odds']['totals'][total] = filtered_total

            source_data.append(source_entry)

        # ä½¿ç”¨å”¯ä¸€é”®å­˜å‚¨æ¯”èµ›æ•°æ®ï¼ˆmatch_name + start_time_beijingï¼‰
        unique_key = f"{match_name}-{start_time_beijing}"
        all_matches_data[unique_key] = {
            "match_name": match_name,
            "league_name": team_mapping['league']['source2'],
            "home_team": team_mapping['home']['source2'],
            "away_team": team_mapping['away']['source2'],
            "start_time_beijing": start_time_beijing,
            "time_until_start": time_until_start,
            "event_id": event_id,  # æ–°å¢å­—æ®µ
            "line_id": line_id,  # æ–°å¢å­—æ®µ
            "league_id": league_id,
            "sources": source_data
        }

    # åœ¨æ„å»ºå®Œæ‰€æœ‰æ¯”èµ›æ•°æ®åè®¡ç®—maxå­—æ®µ
    all_matches_data = calculate_odds_max(all_matches_data)

    # åœ¨æ„å»ºall_matches_dataåè®¡ç®—is189
    for match_data in all_matches_data.values():
        is189_data = calculate_is189(match_data)
        # ç›´æ¥å°†è®¡ç®—ç»“æœåˆå¹¶åˆ°match_dataä¸­
        match_data.update(is189_data)
        match_data["result"] = is189_data.get("result", None)

        # æ–°å¢ï¼šè®¡ç®—å¤§å°çƒç›˜æŒ‡æ•°
        total_189_data = calculate_total_189(match_data)
        match_data.update(total_189_data)
        match_data["total_result"] = total_189_data.get("total_result", None)
        match_data["is_total_189"] = total_189_data.get("is_total_189", False)

    # æ‰“å°æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯ï¼ˆæ ¸å¿ƒï¼šä¿®å¤é™¤ä»¥é›¶é”™è¯¯ï¼‰
    print(f"============================================")
    print(f"ğŸ“Š æ•°æ®å¤„ç†å®Œæˆ:")
    print(f"  - æ€»æ¯”èµ›æ•°: {len(all_matches_data)}")
    # ä¿®å¤ï¼šåˆ¤æ–­total_matchedæ˜¯å¦ä¸º0ï¼Œé¿å…é™¤ä»¥é›¶
    if total_matched > 0:
        print(f"  - æˆåŠŸç‡: {len(all_matches_data) / total_matched * 100:.2f}% (åŸºäºä¸¤æºåŒ¹é…æˆåŠŸæ•°)")
    else:
        print(f"  - æˆåŠŸç‡: 0% (ä¸¤æºåŒ¹é…æˆåŠŸæ•°ä¸º0ï¼Œæ— æ³•è®¡ç®—)")
    if total_matches_source2 > 0:
        print(f"  - æˆåŠŸç‡: {len(all_matches_data) / total_matches_source2 * 100:.2f}% (åŸºäºsource2æ€»æ¯”èµ›æ•°)")
    else:
        print(f"  - æˆåŠŸç‡: 0% (source2æ— æ•°æ®)")
    print(f"============================================")

    # æ–°å¢ï¼šè¿”å›ä¸‰ä¸ªå…³é”®å€¼ï¼Œä¾›ä¸»å‡½æ•°åˆ¤æ–­çŠ¶æ€
    return all_matches_data, total_matched, total_matches_source2


# === æ–°å¢ï¼šè®¡ç®—èµ”ç‡æ•°æ®å“ˆå¸Œ ===
def calculate_odds_hash(match_data: Dict) -> str:
    """è®¡ç®—æ¯”èµ›èµ”ç‡æ•°æ®çš„MD5å“ˆå¸Œå€¼"""
    if not match_data:
        return ""

    # æå–å¹¶åºåˆ—åŒ–èµ”ç‡æ•°æ®
    odds_data = {}
    for source in match_data.get("sources", []):
        source_id = source.get("source")
        odds_data[source_id] = source.get("odds", {})

    # è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²å¹¶æ’åºé”®ï¼Œç¡®ä¿ç›¸åŒèµ”ç‡ç”Ÿæˆç›¸åŒå“ˆå¸Œ
    odds_str = json.dumps(odds_data, sort_keys=True, ensure_ascii=False, default=str).encode('utf-8')
    return md5(odds_str).hexdigest()


# === ä¿®æ”¹åçš„èµ”ç‡å¯¹æ¯”å‡½æ•° ===
def compare_odds(old_data: Dict, new_data: Dict) -> List[Dict]:
    """å¯¹æ¯”ä¸¤ä¸ªæ¯”èµ›çš„èµ”ç‡ï¼Œç‹¬ç«‹è·Ÿè¸ªæ¯ä¸ªæ–¹å‘çš„å˜åŒ–ï¼ˆhome/away åˆ†ç¦»ï¼‰"""
    changes = []
    old_sources = {s["source"]: s["odds"] for s in old_data.get("sources", [])}
    new_sources = {s["source"]: s["odds"] for s in new_data.get("sources", [])}

    for source_id in set(old_sources.keys()) | set(new_sources.keys()):
        old_odds = old_sources.get(source_id, {})
        new_odds = new_sources.get(source_id, {})

        # å¤„ç†è®©åˆ†ç›˜ï¼ˆspreadï¼‰
        process_odds_direction(changes, old_odds, new_odds, source_id, "spreads", "spread", ["home", "away"])
        # å¤„ç†å¤§å°çƒï¼ˆtotalï¼‰
        process_odds_direction(changes, old_odds, new_odds, source_id, "totals", "total", ["over", "under"])

    return changes


def process_odds_direction(
        changes: List[Dict],
        old_odds: Dict,
        new_odds: Dict,
        source_id: int,
        odds_type: str,
        change_type: str,
        directions: List[str]
):
    """ç‹¬ç«‹å¤„ç†æ¯ä¸ªæ–¹å‘çš„èµ”ç‡å˜åŒ–ï¼ˆhome/away æˆ– over/underï¼‰"""
    old_items = old_odds.get(odds_type, {})
    new_items = new_odds.get(odds_type, {})

    for key in set(old_items.keys()) | set(new_items.keys()):
        old_dir_data = old_items.get(key, {})
        new_dir_data = new_items.get(key, {})

        for direction in directions:
            old_value = old_dir_data.get(direction)
            new_value = new_dir_data.get(direction)

            if old_value != new_value:
                # ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„å­—æ®µåï¼šspread_value æˆ– total_value
                value_key = f"{change_type}_value"

                changes.append({
                    "type": change_type,
                    "source": source_id,
                    value_key: key,  # ä½¿ç”¨ spread_value æˆ– total_value
                    "side": direction,
                    "old_value": old_value,
                    "new_value": new_value
                })


# === æ–°å¢ï¼šæ£€æŸ¥APIå“åº”æ˜¯å¦æœ‰å¤±è´¥ ===
def check_api_failures(results: List[Dict[str, Any]]) -> bool:
    """æ£€æŸ¥APIè¯·æ±‚ç»“æœä¸­æ˜¯å¦æœ‰å¤±è´¥çš„æƒ…å†µ"""
    failed_apis = [result["url"] for result in results if result["status"] == "error"]
    if failed_apis:
        print(f"â— æ£€æµ‹åˆ°APIå¤±æ•ˆ: {', '.join(failed_apis)}")
        return True
    return False


# === æ–°å¢ï¼šç»´æŠ¤å…¨å±€æ¯”èµ›æ•°æ®ç¼“å­˜ ===
def update_matches_cache(matches_data: Dict):
    """æ›´æ–°å…¨å±€æ¯”èµ›æ•°æ®ç¼“å­˜ï¼ˆä½¿ç”¨å”¯ä¸€é”®ï¼Œä¸¥æ ¼æ ¡éªŒæ•°æ®å®Œæ•´æ€§ï¼‰"""
    global all_matches_cache

    # æ¸…é™¤æ— æ•ˆé”®ï¼ˆç¡®ä¿é”®ä¸º match_name-start_time_beijing æ ¼å¼ï¼‰
    valid_matches = {}
    for key, data in matches_data.items():
        # æ ¡éªŒé”®æ ¼å¼ï¼ˆå¯é€‰ï¼šç¡®ä¿é”®åŒ…å«åˆ†éš”ç¬¦ï¼‰
        if '-' not in key:
            print(f"âš ï¸ æ— æ•ˆç¼“å­˜é”® {key}ï¼Œæ ¼å¼å¿…é¡»ä¸º match_name-start_time_beijing")
            continue
        # æ ¡éªŒæ•°æ®å®Œæ•´æ€§
        required_fields = ["match_name", "start_time_beijing", "sources"]
        if any(field not in data for field in required_fields):
            print(f"âš ï¸ æ¯”èµ› {key} ç¼ºå°‘å¿…è¦å­—æ®µï¼Œä¸åŠ å…¥ç¼“å­˜")
            continue
        valid_matches[key] = data

    # æ·»åŠ æ—¶é—´æˆ³å¹¶æ›´æ–°ç¼“å­˜
    current_time = datetime.now().isoformat()
    for key in valid_matches:
        valid_matches[key]["last_updated"] = current_time

    all_matches_cache = valid_matches
    print(f"âœ… æ¯”èµ›æ•°æ®ç¼“å­˜å·²æ›´æ–°ï¼Œæœ‰æ•ˆæ•°æ®é‡: {len(all_matches_cache)}")


# === æ–°å¢ï¼šWebSocketå¹¿æ’­å‡½æ•°ï¼ˆä¿®æ”¹ä¸ºæ•°æ®æ›´æ–°åè°ƒç”¨ï¼‰===
async def broadcast_matches_data():
    """å¹¿æ’­å®Œæ•´æ¯”èµ›æ•°æ®ï¼ˆç›´æ¥æ¨é€ç¼“å­˜ä¸­çš„å€¼åˆ—è¡¨ï¼‰"""
    try:
        if connected_clients and all_matches_cache:
            # è½¬æ¢ä¸ºåˆ—è¡¨æ—¶ä¿ç•™å®Œæ•´æ•°æ®ï¼ˆé”®å·²åŒ…å«åœ¨æ•°æ®ä¸­ï¼‰
            matches_list = list(all_matches_cache.values())

            # æ£€æŸ¥æ•°æ®æ ¼å¼ï¼ˆç¡®ä¿åŒ…å«å‰ç«¯æ‰€éœ€å­—æ®µï¼‰
            if any("start_time_beijing" not in m for m in matches_list):
                print("âš ï¸ æ£€æµ‹åˆ°ä¸å®Œæ•´æ¯”èµ›æ•°æ®ï¼Œè·³è¿‡æœ¬æ¬¡å¹¿æ’­")
                return

            data_to_send = {
                "timestamp": datetime.now().isoformat(),
                "matches": matches_list,
                "api_errors": list(current_api_errors),  # å½“å‰å¤±è´¥çš„APIåˆ—è¡¨
                "connection_count": len(connected_clients)  # å½“å‰WebSocketè¿æ¥æ•°
            }
            await asyncio.gather(
                *[client.send(json.dumps(data_to_send, default=str)) for client in connected_clients]
            )
            print(f"ğŸ“¢ å¹¿æ’­ {len(matches_list)} åœºæ¯”èµ›æ•°æ®")
    except Exception as e:
        print(f"âŒ WebSocketå¹¿æ’­å¤±è´¥: {e}")


async def ws_handler(websocket, path):
    """å¤„ç†WebSocketè¿æ¥"""
    # æ·»åŠ å®¢æˆ·ç«¯åˆ°è¿æ¥é›†åˆ
    connected_clients.add(websocket)
    print(f"âœ… æ–°çš„WebSocketè¿æ¥ï¼Œå½“å‰è¿æ¥æ•°: {len(connected_clients)}")
    await broadcast_matches_data()
    try:
        # ä¿æŒè¿æ¥æ‰“å¼€
        await websocket.wait_closed()
    finally:
        # è¿æ¥å…³é—­æ—¶ç§»é™¤å®¢æˆ·ç«¯
        connected_clients.remove(websocket)
        print(f"â„¹ï¸ WebSocketè¿æ¥å·²å…³é—­ï¼Œå½“å‰è¿æ¥æ•°: {len(connected_clients)}")

# === ä¸»å‡½æ•° ===
async def main():
    """ä¸»å‡½æ•°ï¼šå‘¨æœŸæ€§è·å–æ‰€æœ‰APIæ•°æ®å¹¶é€šè¿‡WebSocketæ¨é€æ›´æ–°"""
    global postgres_pool, last_matches_data, current_source1_index  # æ–°å¢current_source1_indexå…¨å±€å˜é‡

    # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± å’Œè¡¨
    if not init_db_pool() or not init_db_tables():
        print("âŒ æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
        return

    try:
        print(f"\n{'=' * 20} ç¨‹åºå¯åŠ¨ï¼Œè·å–åˆå§‹æ•°æ® [{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {'=' * 20}")

        # === WebSocket æœåŠ¡å¯åŠ¨ ===
        ws_server = await websockets.serve(ws_handler, WS_CONFIG["host"], WS_CONFIG["port"])
        print(f"âœ… WebSocketæœåŠ¡å·²å¯åŠ¨: ws://{WS_CONFIG['host']}:{WS_CONFIG['port']}")

        # é¦–æ¬¡æ•°æ®è·å–ä¸åˆå§‹åŒ–
        async with aiohttp.ClientSession() as session:
            # é¦–æ¬¡ä½¿ç”¨SOURCE1_URLS[0]ä½œä¸ºsource1
            current_source1_url = SOURCE1_URLS[current_source1_index]
            print(f"ğŸ“¥ é¦–æ¬¡è·å–ï¼šä½¿ç”¨source1 APIï¼ˆç´¢å¼•{current_source1_index}ï¼‰- {current_source1_url}")
            # æ„å»ºä»»åŠ¡åˆ—è¡¨ï¼ˆsource1ç”¨ç¬¬ä¸€ä¸ªURLï¼Œsource2å›ºå®šï¼‰
            tasks = [
                fetch_api(session, current_source1_url),  # source1ï¼ˆç¬¬ä¸€ä¸ªAPIï¼‰
                fetch_api(session, SOURCE2_URL)           # source2ï¼ˆå›ºå®šï¼‰
            ]
            results = await asyncio.gather(*tasks)
            # æ›´æ–°API_URLSç¡®ä¿åç»­é€»è¾‘å…¼å®¹
            API_URLS[0] = current_source1_url

            current_source1_index = (current_source1_index + 1) % len(SOURCE1_URLS)  # 0â†’1â†’2â†’0å¾ªç¯

            if check_api_failures(results):
                print(f"âš ï¸ ç¨‹åºå°†æš‚åœ {API_FAILURE_DELAY} ç§’åç»§ç»­è¿è¡Œ...")
                await asyncio.sleep(API_FAILURE_DELAY)
                return

            # é¦–æ¬¡å¤„ç†æ•°æ®ï¼šæ¥æ”¶ä¸‰ä¸ªè¿”å›å€¼
            all_matches_data, total_matched, total_matches_source2 = await process_api_data(results)

        if all_matches_data and len(all_matches_data) > 0:
            print("\n" + "=" * 50)
            print(f"ğŸ“¥ åˆå§‹åŒ–ï¼šä¿å­˜åˆå§‹æ¯”èµ›æ•°æ®åˆ°æ•°æ®åº“")
            print("=" * 50)

            for match_name, match_data in all_matches_data.items():
                match_id = save_match_info(match_name, match_data)
                if match_id:
                    dummy_changes = []
                    for source in match_data.get("sources", []):
                        source_id = source.get("source")
                        for spread_key, spread_data in source.get("odds", {}).get("spreads", {}).items():
                            for side, value in spread_data.items():
                                dummy_changes.append({
                                    "type": "spread",
                                    "source": source_id,
                                    "spread_value": spread_key,
                                    "side": side,
                                    "old_value": None,
                                    "new_value": value
                                })
                        for total_key, total_data in source.get("odds", {}).get("totals", {}).items():
                            for side, value in total_data.items():
                                dummy_changes.append({
                                    "type": "total",
                                    "source": source_id,
                                    "total_value": total_key,
                                    "side": side,
                                    "old_value": None,
                                    "new_value": value
                                })
                    if dummy_changes:
                        save_odds_changes(match_id, match_name, dummy_changes)
                        print(f"âœ… å·²ä¿å­˜åˆå§‹æ•°æ®: {match_name} ({len(dummy_changes)} æ¡èµ”ç‡è®°å½•)")

            for match_name, match_data in all_matches_data.items():
                cache_key = (match_name, match_data["start_time_beijing"])
                last_matches_data[cache_key] = (calculate_odds_hash(match_data), match_data)

            update_matches_cache(all_matches_data)

            # åˆå§‹æ•°æ®åŠ è½½åç«‹å³å¹¿æ’­
            await broadcast_matches_data()

            print(f"\nâœ… åˆå§‹æ•°æ®ä¿å­˜å®Œæˆï¼Œå…± {len(all_matches_data)} åœºæ¯”èµ›")
        else:
            print("â„¹ï¸ åˆå§‹æ•°æ®ä¸ºç©ºï¼Œç¨‹åºå°†ç»§ç»­è¿è¡Œä½†æ— æ•°æ®å¯ä¿å­˜")

        # ä¸»å¾ªç¯ï¼šå‘¨æœŸæ€§æ•°æ®å¤„ç†
        print(f"\n{'=' * 20} å¼€å§‹å‘¨æœŸæ€§æ•°æ®è·å– [{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {'=' * 20}")

        # æ–°å¢ï¼šæ§åˆ¶ç­‰å¾…é—´éš”ï¼ˆé»˜è®¤1ç§’ï¼Œæ— åŒ¹é…æ—¶5åˆ†é’Ÿï¼‰
        fetch_interval = 10

        while True:
            try:
                start_time = time.time()
                print(f"\n{'=' * 20} å¼€å§‹æ–°ä¸€è½®æ•°æ®è·å– [{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {'=' * 20}")

                # è·å–æœ€æ–°æ•°æ®ï¼ˆè½®æ¢source1 APIï¼‰
                async with aiohttp.ClientSession() as session:
                    # æ ¹æ®å½“å‰ç´¢å¼•é€‰æ‹©source1 URL
                    current_source1_url = SOURCE1_URLS[current_source1_index]
                    print(f"ğŸ“¥ æœ¬è½®è·å–ï¼šä½¿ç”¨source1 APIï¼ˆç´¢å¼•{current_source1_index}ï¼‰- {current_source1_url}")
                    # æ„å»ºä»»åŠ¡åˆ—è¡¨ï¼ˆsource1è½®æ¢ï¼Œsource2å›ºå®šï¼‰
                    tasks = [
                        fetch_api(session, current_source1_url),  # è½®æ¢çš„source1
                        fetch_api(session, SOURCE2_URL)           # å›ºå®šçš„source2
                    ]
                    results = await asyncio.gather(*tasks)
                    # æ›´æ–°API_URLSç¡®ä¿åç»­é€»è¾‘å…¼å®¹
                    API_URLS[0] = current_source1_url

                    current_source1_index = (current_source1_index + 1) % len(SOURCE1_URLS)  # æ ¸å¿ƒä¿®æ”¹

                global current_api_errors
                current_api_errors = {result["url"] for result in results if result["status"] == "error"}

                # æ— è®ºæ˜¯å¦å¤±è´¥ï¼Œéƒ½å¹¿æ’­å½“å‰çŠ¶æ€
                await broadcast_matches_data()  # æå‰å¹¿æ’­çŠ¶æ€ï¼Œç¡®ä¿é”™è¯¯åŠæ—¶æ˜¾ç¤º

                if check_api_failures(results):
                    print(f"âš ï¸ æ£€æµ‹åˆ°APIè¯·æ±‚å¤±è´¥ï¼Œè·³è¿‡æ­¤è½®æ•°æ®å¤„ç†")
                    await asyncio.sleep(fetch_interval)
                    continue

                # æ ¸å¿ƒï¼šæ¥æ”¶å¤„ç†åçš„æ•°æ®å’ŒåŒ¹é…æ•°ï¼ˆtotal_matchedæ˜¯å…³é”®ï¼‰
                all_matches_data, total_matched, total_matches_source2 = await process_api_data(results)

                # æ ¸å¿ƒé€»è¾‘ï¼šå¦‚æœä¸¤æºåŒ¹é…æˆåŠŸæ•°ä¸º0ï¼Œè¿›å…¥5åˆ†é’Ÿå¾…æœº
                if total_matched == 0:
                    print(f"âš ï¸ ä¸¤æºåŒ¹é…æˆåŠŸæ•°ä¸º0ï¼Œè¿›å…¥å¾…æœºçŠ¶æ€ï¼ˆæ¯5åˆ†é’Ÿé‡è¯•ä¸€æ¬¡ï¼‰")
                    fetch_interval = 300  # åˆ‡æ¢ä¸º5åˆ†é’Ÿ
                    await asyncio.sleep(fetch_interval)
                    continue
                else:
                    # æœ‰åŒ¹é…æˆåŠŸçš„æ•°æ®ï¼Œæ¢å¤æ­£å¸¸é—´éš”ï¼ˆ1ç§’ï¼‰
                    fetch_interval = 10

                if not all_matches_data:
                    print("â„¹ï¸ æœ¬è½®è·å–çš„æ¯”èµ›æ•°æ®ä¸ºç©º")
                    await asyncio.sleep(fetch_interval)
                    continue

                # æ•°æ®å¯¹æ¯”ä¸å˜åŒ–æ£€æµ‹ï¼ˆä¿æŒä¸å˜ï¼‰
                new_matches = []  # æ–°å¢æ¯”èµ›
                changed_matches = []  # èµ”ç‡å˜åŒ–çš„æ¯”èµ›
                removed_matches = []  # ç§»é™¤çš„æ¯”èµ›
                detailed_changes = {}  # è¯¦ç»†èµ”ç‡å˜åŒ–

                # ä½¿ç”¨match_name + start_time_beijingä½œä¸ºå”¯ä¸€æ ‡è¯†
                current_cache_keys = {(match_name, data["start_time_beijing"]) for match_name, data in
                                      all_matches_data.items()}
                previous_cache_keys = set(last_matches_data.keys())

                # æ£€æŸ¥æ–°å¢æ¯”èµ›
                for cache_key in current_cache_keys - previous_cache_keys:
                    match_name, _ = cache_key
                    if match_name in all_matches_data:
                        new_matches.append(match_name)
                        current_data = all_matches_data[match_name]
                        current_hash = calculate_odds_hash(current_data)
                        last_matches_data[cache_key] = (current_hash, current_data)

                        # ä¿å­˜æ–°æ¯”èµ›åˆ°æ•°æ®åº“
                        match_id = save_match_info(match_name, current_data)
                        if match_id:
                            # æå–æ‰€æœ‰èµ”ç‡ä½œä¸º"å˜åŒ–"ä¿å­˜
                            initial_changes = []
                            for source in current_data.get("sources", []):
                                source_id = source.get("source")
                                for spread_key, spread_data in source.get("odds", {}).get("spreads", {}).items():
                                    for side, value in spread_data.items():
                                        initial_changes.append({
                                            "type": "spread",
                                            "source": source_id,
                                            "spread_value": spread_key,
                                            "side": side,
                                            "old_value": None,
                                            "new_value": value
                                        })
                                for total_key, total_data in source.get("odds", {}).get("totals", {}).items():
                                    for side, value in total_data.items():
                                        initial_changes.append({
                                            "type": "total",
                                            "source": source_id,
                                            "total_value": total_key,
                                            "side": side,
                                            "old_value": None,
                                            "new_value": value
                                        })
                            if initial_changes:
                                save_odds_changes(match_id, match_name, initial_changes)
                                print(f"âœ… å·²ä¿å­˜æ–°æ¯”èµ›æ•°æ®: {match_name} ({len(initial_changes)} æ¡èµ”ç‡è®°å½•)")

                # æ£€æŸ¥èµ”ç‡å˜åŒ–
                for cache_key in current_cache_keys & previous_cache_keys:
                    match_name, _ = cache_key
                    current_data = all_matches_data[match_name]
                    current_hash = calculate_odds_hash(current_data)
                    previous_hash, previous_data = last_matches_data[cache_key]

                    if current_hash != previous_hash:
                        changed_matches.append(match_name)
                        last_matches_data[cache_key] = (current_hash, current_data)

                        # è®¡ç®—è¯¦ç»†å˜åŒ–
                        changes = compare_odds(previous_data, current_data)
                        if changes:
                            detailed_changes[match_name] = changes

                            # ä¿å­˜å˜åŒ–åˆ°æ•°æ®åº“
                            match_id = save_match_info(match_name, current_data)
                            if match_id:
                                save_odds_changes(match_id, match_name, changes)

                # æ£€æŸ¥ç§»é™¤çš„æ¯”èµ›
                for cache_key in previous_cache_keys - current_cache_keys:
                    match_name, _ = cache_key
                    if cache_key in last_matches_data:
                        removed_matches.append(match_name)
                        del last_matches_data[cache_key]

                # æ›´æ–°å…¨å±€ç¼“å­˜
                update_matches_cache(all_matches_data)

                # æ•°æ®æ›´æ–°å®Œæˆåç«‹å³å¹¿æ’­
                await broadcast_matches_data()

                # æ‰“å°å˜åŒ–ç»Ÿè®¡
                print("\n" + "=" * 50)
                print(f"ğŸ“Š æ•°æ®å˜åŒ–ç»Ÿè®¡")
                print("=" * 50)
                print(f"  - æ–°å¢æ¯”èµ›: {len(new_matches)}")
                print(f"  - èµ”ç‡å˜åŒ–: {len(changed_matches)}")
                print(f"  - ç§»é™¤æ¯”èµ›: {len(removed_matches)}")

                # æ‰“å°æ–°å¢æ¯”èµ›
                if new_matches:
                    print("\nğŸ“ˆ æ–°å¢æ¯”èµ›:")
                    for match_name in new_matches:
                        print(f"  - {match_name}")

                # æ‰“å°è¯¦ç»†èµ”ç‡å˜åŒ–
                if detailed_changes:
                    print("\nğŸ“Š è¯¦ç»†èµ”ç‡å˜åŒ–:")
                    for match_name, changes in detailed_changes.items():
                        print(f"\n  - {match_name}")
                        for change in changes:
                            if change["type"] == "spread":
                                print(
                                    f"    ğŸ”¹ æ•°æ®æº{change['source']} è®©åˆ†ç›˜ {change['spread_value']} - {change['side']}: {change['old_value']} â†’ {change['new_value']}")
                            else:
                                print(
                                    f"    ğŸ”¹ æ•°æ®æº{change['source']} å¤§å°çƒ {change['total_value']} - {change['side']}: {change['old_value']} â†’ {change['new_value']}")

                # æ‰“å°ç§»é™¤æ¯”èµ›
                if removed_matches:
                    print("\nâŒ ç§»é™¤æ¯”èµ›:")
                    for match_name in removed_matches:
                        print(f"  - {match_name}")

                if not new_matches and not changed_matches and not removed_matches:
                    print("\nâ„¹ï¸ æ— æ•°æ®å˜åŒ–")

                # è®¡ç®—å¤„ç†æ—¶é—´å’Œä¸‹ä¸€æ¬¡è·å–æ—¶é—´
                elapsed = time.time() - start_time
                print(f"\n{'=' * 50}")
                print(f"ğŸ“Š æœ¬è½®æ•°æ®å¤„ç†å®Œæˆ")
                print(f"  - å¤„ç†æ—¶é—´: {elapsed:.2f}ç§’")
                print(f"  - ä¸‹æ¬¡æ•°æ®è·å–å°†åœ¨{fetch_interval}ç§’åè¿›è¡Œ")
                print(f"{'=' * 50}\n")

                # ç­‰å¾…ä¸‹ä¸€ä¸ªå‘¨æœŸ
                await asyncio.sleep(fetch_interval)

            except Exception as e:
                print(f"âŒ å‘¨æœŸæ•°æ®è·å–å¼‚å¸¸: {e}")
                # è®°å½•å®Œæ•´å †æ ˆè·Ÿè¸ª
                import traceback
                traceback.print_exc()
                # ç­‰å¾…ä¸€æ®µæ—¶é—´å†é‡è¯•
                await asyncio.sleep(5)

    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç”¨æˆ·æ‰‹åŠ¨ç»ˆæ­¢ç¨‹åº")
    finally:
        # èµ„æºæ¸…ç†
        if 'ws_server' in locals():
            ws_server.close()
            await ws_server.wait_closed()

        # å…³é—­æ•°æ®åº“è¿æ¥æ± 
        if postgres_pool:
            postgres_pool.closeall()
            print("âœ… æ•°æ®åº“è¿æ¥æ± å·²å…³é—­")

        print("ğŸ‘‹ ç¨‹åºå·²é€€å‡º")


if __name__ == "__main__":
    if hasattr(asyncio, 'WindowsSelectorEventLoopPolicy'):
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

    asyncio.run(main())