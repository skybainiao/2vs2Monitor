import asyncio
import aiohttp
import json
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Tuple
import psycopg2
from psycopg2 import pool
from psycopg2.extras import DictCursor
import functools
import time
from collections import defaultdict, ChainMap
from hashlib import md5  # ç”¨äºæ•°æ®å“ˆå¸Œå¯¹æ¯”
# å¯¼å…¥WebSocketåº“
import websockets




# === é…ç½®åŒº ===
# APIé…ç½®
API_URLS = [
    "http://127.0.0.1:5001/get_odds1",
    "http://127.0.0.1:5002/get_odds2",
    "http://127.0.0.1:5003/get_odds3"
]

# æ•°æ®åº“é…ç½®
DB_CONFIG = {
    "host": "localhost",
    "database": "postgres",
    "user": "postgres",
    "password": "cjj2468830035",
    "port": 5432
}

# è¿æ¥æ± é…ç½®
DB_POOL_CONFIG = {
    "minconn": 2,  # æœ€å°è¿æ¥æ•°
    "maxconn": 10,  # æœ€å¤§è¿æ¥æ•°
    **DB_CONFIG  # ç»§æ‰¿åŸºç¡€æ•°æ®åº“é…ç½®
}

# é‡è¯•é…ç½®
MAX_RETRIES = 3
RETRY_DELAY = 2  # ç§’

# å®šæ—¶ä»»åŠ¡é…ç½®
FETCH_INTERVAL = 30  # ç§’

# APIå¤±æ•ˆå¤„ç†é…ç½®
API_FAILURE_DELAY = 60  # å¤±æ•ˆåæš‚åœæ—¶é—´ï¼ˆç§’ï¼‰

# === å…¨å±€å˜é‡ ===
postgres_pool = None  # æ•°æ®åº“è¿æ¥æ± 
last_matches_data = {}  # ä¸Šæ¬¡çš„æ¯”èµ›æ•°æ®ç¼“å­˜ {match_name: (hash, data)}

# === æ–°å¢ï¼šå…¨å±€æ¯”èµ›æ•°æ®ç¼“å­˜ ===
all_matches_cache = {}  # æ‰€æœ‰æ¯”èµ›çš„æœ€æ–°æ•°æ®ç¼“å­˜ {match_name: data}


# === æ–°å¢ï¼šWebSocketç›¸å…³é…ç½® ===
WS_CONFIG = {
    "host": "160.25.20.18",
    "port": 8765
}

# å­˜å‚¨æ‰€æœ‰è¿æ¥çš„å®¢æˆ·ç«¯
connected_clients = set()

# === è£…é¥°å™¨ ===
def timed(func):
    """å‡½æ•°æ‰§è¡Œæ—¶é—´è£…é¥°å™¨"""

    @functools.wraps(func)
    async def wrapper(*args, **kwargs):
        start_time = time.perf_counter()
        result = await func(*args, **kwargs)
        end_time = time.perf_counter()
        print(f"â±ï¸ {func.__name__} æ‰§è¡Œæ—¶é—´: {(end_time - start_time) * 1000:.2f}ms")
        return result

    return wrapper


# === APIè¯·æ±‚æ¨¡å— ===
async def fetch_api(session, url, retries=MAX_RETRIES):
    """å¼‚æ­¥è·å–APIæ•°æ®ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
    for attempt in range(retries):
        start_time = datetime.now()
        try:
            async with session.get(url) as response:
                elapsed = (datetime.now() - start_time).total_seconds() * 1000  # æ¯«ç§’
                if response.status == 200:
                    data = await response.json(content_type=None)  # å¤„ç†éæ ‡å‡†JSONå“åº”
                    return {
                        "url": url,
                        "status": "success",
                        "data": data,
                        "timestamp": datetime.now().isoformat(),
                        "response_time": elapsed
                    }
        except Exception as e:
            pass
        if attempt < retries - 1:
            await asyncio.sleep(RETRY_DELAY)
    return {
        "url": url,
        "status": "error",
        "error_message": f"è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({retries})",
        "timestamp": datetime.now().isoformat(),
        "response_time": (datetime.now() - start_time).total_seconds() * 1000
    }


# === æ•°æ®åº“è¿æ¥æ± æ¨¡å— ===
def init_db_pool():
    """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± """
    global postgres_pool
    try:
        postgres_pool = pool.SimpleConnectionPool(**DB_POOL_CONFIG)
        print(
            f"âœ… æ•°æ®åº“è¿æ¥æ± åˆå§‹åŒ–æˆåŠŸï¼Œæœ€å°è¿æ¥æ•°: {DB_POOL_CONFIG['minconn']}ï¼Œæœ€å¤§è¿æ¥æ•°: {DB_POOL_CONFIG['maxconn']}")
        return True
    except Exception as e:
        print(f"âŒ æ•°æ®åº“è¿æ¥æ± åˆå§‹åŒ–å¤±è´¥: {e}")
        return False


def get_db_connection():
    """ä»è¿æ¥æ± è·å–æ•°æ®åº“è¿æ¥"""
    if postgres_pool is None:
        if not init_db_pool():
            return None
    try:
        return postgres_pool.getconn()
    except Exception as e:
        print(f"âŒ ä»è¿æ¥æ± è·å–è¿æ¥å¤±è´¥: {e}")
        return None


def release_db_connection(conn):
    """å°†æ•°æ®åº“è¿æ¥é‡Šæ”¾å›è¿æ¥æ± """
    if conn and postgres_pool:
        try:
            postgres_pool.putconn(conn)
        except Exception as e:
            print(f"âŒ é‡Šæ”¾è¿æ¥å›æ± å¤±è´¥: {e}")


# === æ–°å¢ï¼šåˆå§‹åŒ–æ•°æ®åº“è¡¨ ===
def init_db_tables():
    """åˆ›å»ºå¿…è¦çš„æ•°æ®åº“è¡¨"""
    conn = get_db_connection()
    if not conn:
        return False

    try:
        with conn.cursor() as cursor:
            # åˆ›å»ºæ¯”èµ›ä¿¡æ¯è¡¨ï¼ˆæ·»åŠ start_time_beijingåˆ°å”¯ä¸€çº¦æŸï¼‰
            cursor.execute("""
            CREATE TABLE IF NOT EXISTS matches (
                id SERIAL PRIMARY KEY,
                match_name TEXT NOT NULL,
                league_name TEXT NOT NULL,
                home_team TEXT NOT NULL,
                away_team TEXT NOT NULL,
                start_time_beijing TEXT NOT NULL,  -- æ–°å¢éç©ºçº¦æŸ
                time_until_start TEXT,
                CONSTRAINT unique_match_time UNIQUE (match_name, start_time_beijing)  -- æ˜¾å¼å‘½åçº¦æŸ
            )
            """)

            # åˆ›å»ºèµ”ç‡å˜åŒ–è®°å½•è¡¨ - è®©åˆ†ç›˜
            cursor.execute("""
            CREATE TABLE IF NOT EXISTS spread_odds (
                id SERIAL PRIMARY KEY,
                match_id INTEGER NOT NULL,
                source INTEGER NOT NULL,
                spread_value TEXT NOT NULL,
                side TEXT NOT NULL,  -- 'home' æˆ– 'away'
                odds_value NUMERIC(5,2) NOT NULL,
                recorded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (match_id) REFERENCES matches (id)
            )
            """)

            # åˆ›å»ºèµ”ç‡å˜åŒ–è®°å½•è¡¨ - å¤§å°çƒ
            cursor.execute("""
            CREATE TABLE IF NOT EXISTS total_odds (
                id SERIAL PRIMARY KEY,
                match_id INTEGER NOT NULL,
                source INTEGER NOT NULL,
                total_value TEXT NOT NULL,
                side TEXT NOT NULL,  -- 'over' æˆ– 'under'
                odds_value NUMERIC(5,2) NOT NULL,
                recorded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (match_id) REFERENCES matches (id)
            )
            """)

            # åˆ›å»ºç´¢å¼•ä»¥åŠ é€ŸæŸ¥è¯¢ï¼ˆåŒ…å«start_time_beijingï¼‰
            cursor.execute(
                "CREATE INDEX IF NOT EXISTS idx_matches_name_time ON matches (match_name, start_time_beijing)")
            cursor.execute(
                "CREATE INDEX IF NOT EXISTS idx_spread_odds ON spread_odds (match_id, source, spread_value, side, recorded_at)")
            cursor.execute(
                "CREATE INDEX IF NOT EXISTS idx_total_odds ON total_odds (match_id, source, total_value, side, recorded_at)")

            conn.commit()
            print("âœ… æ•°æ®åº“è¡¨åˆå§‹åŒ–æˆåŠŸ")
            return True
    except Exception as e:
        print(f"âŒ æ•°æ®åº“è¡¨åˆå§‹åŒ–å¤±è´¥: {e}")
        conn.rollback()
        return False
    finally:
        release_db_connection(conn)


# === æ–°å¢ï¼šå°†æ¯”èµ›ä¿¡æ¯å­˜å…¥æ•°æ®åº“ï¼ˆåŒ…å«start_time_beijingï¼‰ ===
def save_match_info(match_name: str, match_data: Dict) -> Optional[int]:
    """ä¿å­˜æ¯”èµ›åŸºæœ¬ä¿¡æ¯åˆ°æ•°æ®åº“ï¼Œè¿”å›match_idï¼ˆä½¿ç”¨match_name + start_time_beijingä½œä¸ºå”¯ä¸€æ ‡è¯†ï¼‰"""
    conn = get_db_connection()
    if not conn:
        return None

    try:
        with conn.cursor() as cursor:
            # æ’å…¥æˆ–æ›´æ–°æ¯”èµ›ä¿¡æ¯ï¼ˆåŸºäºmatch_nameå’Œstart_time_beijingï¼‰
            cursor.execute("""
            INSERT INTO matches (match_name, league_name, home_team, away_team, start_time_beijing, time_until_start)
            VALUES (%s, %s, %s, %s, %s, %s)
            ON CONFLICT (match_name, start_time_beijing) DO UPDATE
            SET league_name = EXCLUDED.league_name,
                home_team = EXCLUDED.home_team,
                away_team = EXCLUDED.away_team,
                time_until_start = EXCLUDED.time_until_start
            RETURNING id
            """, (
                match_name,
                match_data["league_name"],
                match_data["home_team"],
                match_data["away_team"],
                match_data["start_time_beijing"],  # å¿…é¡»éç©º
                match_data["time_until_start"]
            ))

            match_id = cursor.fetchone()[0]
            conn.commit()
            return match_id
    except Exception as e:
        print(f"âŒ ä¿å­˜æ¯”èµ›ä¿¡æ¯å¤±è´¥: {e}")
        conn.rollback()
        return None
    finally:
        release_db_connection(conn)


# === æ–°å¢ï¼šå°†èµ”ç‡å˜åŒ–å­˜å…¥æ•°æ®åº“ ===
def save_odds_changes(match_id: int, match_name: str, changes: List[Dict]):
    """ä¿å­˜èµ”ç‡å˜åŒ–åˆ°æ•°æ®åº“"""
    if not changes:
        return

    conn = get_db_connection()
    if not conn:
        return

    try:
        with conn.cursor() as cursor:
            for change in changes:
                if change["type"] == "spread":
                    # ä¿å­˜è®©åˆ†ç›˜å˜åŒ–
                    cursor.execute("""
                    INSERT INTO spread_odds (match_id, source, spread_value, side, odds_value)
                    VALUES (%s, %s, %s, %s, %s)
                    """, (
                        match_id,
                        change["source"],
                        change["spread"],
                        change["side"],
                        change["new_value"]
                    ))
                else:
                    # ä¿å­˜å¤§å°çƒå˜åŒ–
                    cursor.execute("""
                    INSERT INTO total_odds (match_id, source, total_value, side, odds_value)
                    VALUES (%s, %s, %s, %s, %s)
                    """, (
                        match_id,
                        change["source"],
                        change["total"],
                        change["side"],
                        change["new_value"]
                    ))

            conn.commit()
            print(f"âœ… å·²ä¿å­˜ {match_name} çš„ {len(changes)} æ¡èµ”ç‡å˜åŒ–è®°å½•")
    except Exception as e:
        print(f"âŒ ä¿å­˜èµ”ç‡å˜åŒ–å¤±è´¥: {e}")
        conn.rollback()
    finally:
        release_db_connection(conn)


# === æ•°æ®å¤„ç†æ¨¡å—ï¼ˆæœªä¿®æ”¹ï¼‰===
def batch_fetch_bindings(league_names: List[str]) -> Dict[str, List[Dict[str, Any]]]:
    """æ‰¹é‡è·å–å¤šä¸ªè”èµ›çš„bindingsæ•°æ®"""
    if not league_names:
        return {}
    conn = get_db_connection()
    if not conn:
        return {}
    league_bindings = defaultdict(list)
    try:
        with conn.cursor(cursor_factory=DictCursor) as cursor:
            query = """
            SELECT * FROM bindings 
            WHERE source3_league = ANY(%s)
            """
            cursor.execute(query, (list(league_names),))
            for binding in cursor.fetchall():
                league = binding['source3_league']
                league_bindings[league].append({
                    "source1_league": binding['source1_league'],
                    "source1_home_team": binding['source1_home_team'],
                    "source1_away_team": binding['source1_away_team'],
                    "source2_league": binding['source2_league'],
                    "source2_home_team": binding['source2_home_team'],
                    "source2_away_team": binding['source2_away_team'],
                    "source3_league": binding['source3_league'],
                    "source3_home_team": binding['source3_home_team'],
                    "source3_away_team": binding['source3_away_team']
                })
    except Exception as e:
        print(f"âŒ æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {e}")
    finally:
        release_db_connection(conn)
    return league_bindings


def create_team_mapping_cache(bindings: List[Dict[str, Any]]) -> Dict[str, Dict[str, str]]:
    """åˆ›å»ºçƒé˜Ÿæ˜ å°„ç¼“å­˜ï¼Œä»…åŒ…å«source1å’Œsource2å‡æœ‰å€¼çš„è®°å½•"""
    mapping_cache = {}
    for binding in bindings:
        home_team = binding['source3_home_team']
        away_team = binding['source3_away_team']
        if binding['source1_home_team'] and binding['source2_home_team']:
            mapping_cache[home_team] = {
                "source1": binding['source1_home_team'],
                "source2": binding['source2_home_team']
            }
        if binding['source1_away_team'] and binding['source2_away_team']:
            mapping_cache[away_team] = {
                "source1": binding['source1_away_team'],
                "source2": binding['source2_away_team']
            }
    return mapping_cache


def create_api_index(api_data: List[Dict[str, Any]]) -> Dict[Tuple[str, str, str], Dict[str, Any]]:
    """åˆ›å»ºAPIæ•°æ®ç´¢å¼•ï¼ŒåŠ é€ŸæŸ¥æ‰¾"""
    return {(match["league_name"], match["home_team"], match["away_team"]): match for match in api_data}


async def process_league(league_name: str, matches: List[Dict[str, Any]],
                         all_api_indexes: Dict[int, Dict[Tuple[str, str, str], Dict[str, Any]]],
                         league_bindings: List[Dict[str, Any]]):
    """å¤„ç†å•ä¸ªè”èµ›çš„æ¯”èµ›ï¼Œå¿…é¡»åŒæ—¶åŒ¹é…source1ã€source2ã€source3æ‰è§†ä¸ºæˆåŠŸ"""
    if not league_bindings:
        return []
    team_mapping_cache = create_team_mapping_cache(league_bindings)
    if not team_mapping_cache:
        return []
    league_info = {
        "source1": league_bindings[0]["source1_league"],
        "source2": league_bindings[0]["source2_league"],
        "source3": league_name
    }
    results = []
    required_sources = {1, 2, 3}
    for match in matches:
        home_team = match["home_team"]
        away_team = match["away_team"]
        if home_team not in team_mapping_cache or away_team not in team_mapping_cache:
            continue
        home_mapping = team_mapping_cache[home_team]
        away_mapping = team_mapping_cache[away_team]
        matched_apis = {}
        missing_sources = set(required_sources)
        for source_index in required_sources:
            if source_index == 3:
                db_key = (league_info["source3"], home_team, away_team)
            else:
                league_key = league_info[f"source{source_index}"]
                home_key = home_mapping[f"source{source_index}"]
                away_key = away_mapping[f"source{source_index}"]
                db_key = (league_key, home_key, away_key)
            api_index = all_api_indexes.get(source_index, {})
            if db_key in api_index:
                matched_apis[source_index] = api_index[db_key]
                missing_sources.discard(source_index)
        if not missing_sources:
            results.append((match, {"home": home_mapping, "away": away_mapping, "league": league_info}, matched_apis))
    return results


# === æ ¸å¿ƒæ•°æ®å¤„ç†ï¼ˆæœªä¿®æ”¹ï¼‰===
@timed
async def process_api_data(results: List[Dict[str, Any]]):
    """Process API data and cross-match with database data"""
    all_api_data = {}
    all_api_indexes = {}

    for i, url in enumerate(API_URLS, 1):
        result = next((r for r in results if r["url"] == url), {"status": "error"})
        if result["status"] == "success":
            all_api_data[i] = result["data"]
            all_api_indexes[i] = create_api_index(result["data"])
        else:
            all_api_data[i] = []
            all_api_indexes[i] = {}

    source3_result = next((r for r in results if r["url"] == API_URLS[2]), None)
    if not source3_result or source3_result["status"] != "success":
        return None

    source3_data = source3_result["data"]
    total_matches_source3 = len(source3_data)

    league_groups = defaultdict(list)
    for match in source3_data:
        league_groups[match["league_name"]].append(match)

    league_bindings_map = batch_fetch_bindings(list(league_groups.keys()))

    tasks = []
    for league_name, matches in league_groups.items():
        tasks.append(process_league(
            league_name,
            matches,
            all_api_indexes,
            league_bindings_map.get(league_name, [])
        ))

    league_results = await asyncio.gather(*tasks)

    all_matched_matches = [match for league_result in league_results for match in league_result]
    total_matched = len(all_matched_matches)

    # ç»Ÿè®¡ä¿¡æ¯æ‰“å°
    print("\n" + "=" * 50)
    print(f"ğŸ“Š Final Matching Statistics")
    print("=" * 50)
    print(f"  - Total Source3 Matches: {total_matched}")
    print(f"  - Matches with Complete Database Bindings: {sum(len(matches) for matches in league_groups.values())}")
    print(f"  - Successfully Matched Matches: {total_matched}")
    print(
        f"  - Matching Success Rate: {total_matched / total_matches_source3 * 100:.2f}% (Based on Source3 total matches)")
    print(
        f"  - Post-Binding Success Rate: {total_matched / (sum(len(matches) for matches in league_groups.values()) or 1) * 100:.2f}% (Based on successfully bound matches)")

    # ç”¨äºå­˜å‚¨æ‰€æœ‰æ¯”èµ›çš„æ•°æ®
    all_matches_data = {}

    for match, team_mapping, matched_apis in all_matched_matches:
        # æå–ä¸‰ä¸ªæ•°æ®æºçš„èµ”ç‡æ•°æ®
        source1_odds = matched_apis[1].get('odds', {'spreads': {}, 'totals': {}})
        source2_odds = matched_apis[2].get('odds', {'spreads': {}, 'totals': {}})
        source3_odds = matched_apis[3].get('odds', {'spreads': {}, 'totals': {}})

        # ---------------------- 1. è®¡ç®—ç›˜å£æ•°å€¼äº¤é›† ----------------------
        common_spreads = set(source1_odds['spreads'].keys()) & \
                         set(source2_odds['spreads'].keys()) & \
                         set(source3_odds['spreads'].keys())

        common_totals = set(source1_odds['totals'].keys()) & \
                        set(source2_odds['totals'].keys()) & \
                        set(source3_odds['totals'].keys())

        # ---------------------- 2. ä¸ºæ¯ä¸ªç›˜å£æ•°å€¼ç‹¬ç«‹è®¡ç®—é”®äº¤é›† ----------------------
        def get_spread_side_keys(odds_data, spread_key):
            """è·å–å•ä¸ªè®©åˆ†ç›˜å£çš„é”®"""
            return set(odds_data['spreads'].get(spread_key, {}).keys())

        def get_total_side_keys(odds_data, total_key):
            """è·å–å•ä¸ªå¤§å°çƒç›˜å£çš„é”®"""
            return set(odds_data['totals'].get(total_key, {}).keys())

        # æ„å»ºè®©åˆ†ç›˜å£çš„é”®äº¤é›†æ˜ å°„ï¼š{spread_key: common_side_keys}
        spread_key_intersections = {}
        for spread_key in common_spreads:
            s1_keys = get_spread_side_keys(source1_odds, spread_key)
            s2_keys = get_spread_side_keys(source2_odds, spread_key)
            s3_keys = get_spread_side_keys(source3_odds, spread_key)
            spread_key_intersections[spread_key] = s1_keys & s2_keys & s3_keys

        # æ„å»ºå¤§å°çƒç›˜å£çš„é”®äº¤é›†æ˜ å°„ï¼š{total_key: common_side_keys}
        total_key_intersections = {}
        for total_key in common_totals:
            s1_keys = get_total_side_keys(source1_odds, total_key)
            s2_keys = get_total_side_keys(source2_odds, total_key)
            s3_keys = get_total_side_keys(source3_odds, total_key)
            total_key_intersections[total_key] = s1_keys & s2_keys & s3_keys

        # ---------------------- 3. ç‹¬ç«‹è¿‡æ»¤æ¯ä¸ªç›˜å£çš„é”® ----------------------
        def filter_odds(odds_data):
            """æŒ‰ç›˜å£æ•°å€¼ç‹¬ç«‹è¿‡æ»¤é”®"""
            filtered_spreads = {}
            for spread_key in common_spreads:
                # è·å–è¯¥ç›˜å£çš„ä¸‰æ–¹å…±æœ‰é”®
                common_keys = spread_key_intersections.get(spread_key, set())
                spread_info = odds_data['spreads'].get(spread_key, {})
                filtered_spreads[spread_key] = {
                    k: v for k, v in spread_info.items() if k in common_keys
                }

            filtered_totals = {}
            for total_key in common_totals:
                # è·å–è¯¥ç›˜å£çš„ä¸‰æ–¹å…±æœ‰é”®
                common_keys = total_key_intersections.get(total_key, set())
                total_info = odds_data['totals'].get(total_key, {})
                filtered_totals[total_key] = {
                    k: v for k, v in total_info.items() if k in common_keys
                }

            return {
                "spreads": filtered_spreads,
                "totals": filtered_totals
            }

        # åº”ç”¨è¿‡æ»¤
        filtered_odds = {
            1: filter_odds(source1_odds),
            2: filter_odds(source2_odds),
            3: filter_odds(source3_odds)
        }

        # ä½¿ç”¨æ•°æ®æº2çš„åç§°ä½œä¸ºæ¯”èµ›åç§°
        match_name = f"{team_mapping['league']['source2']} - {team_mapping['home']['source2']} vs {team_mapping['away']['source2']}"

        # æ„å»ºæ¯ä¸ªæ•°æ®æºçš„æ•°æ®
        source_data = []
        for source_index in sorted(matched_apis.keys()):
            api_match = matched_apis[source_index]

            # å¤„ç†ä¸»å®¢é˜Ÿåç§°
            if source_index == 3:
                home_team = api_match['home_team']
                away_team = api_match['away_team']
                league_name = api_match['league_name']
            else:
                source_key = f"source{source_index}"
                home_team = team_mapping["home"][source_key]
                away_team = team_mapping["away"][source_key]
                league_name = team_mapping["league"][source_key]

            # æ„å»ºå•ä¸ªæ•°æ®æºçš„æ•°æ®
            source_entry = {
                "source": source_index,
                "league": league_name,
                "home_team": home_team,
                "away_team": away_team,
                "odds": filtered_odds[source_index]
            }
            source_data.append(source_entry)

        # ä»æ•°æ®æº1çš„åŸå§‹æ•°æ®ä¸­è·å–start_time_beijingå’Œtime_until_start
        source1_match_key = (team_mapping['league']['source1'],
                             team_mapping['home']['source1'],
                             team_mapping['away']['source1'])

        # æŸ¥æ‰¾æ•°æ®æº1ä¸­å¯¹åº”çš„æ¯”èµ›
        source1_raw_match = None
        for match in all_api_data[1]:
            if (match.get('league_name') == source1_match_key[0] and
                    match.get('home_team') == source1_match_key[1] and
                    match.get('away_team') == source1_match_key[2]):
                source1_raw_match = match
                break

        # æå–æ‰€éœ€å­—æ®µï¼ˆç¡®ä¿start_time_beijingéç©ºï¼‰
        start_time_beijing = source1_raw_match.get('start_time_beijing', '') if source1_raw_match else ''
        if not start_time_beijing:
            print(f"è­¦å‘Š: æ¯”èµ› {match_name} çš„start_time_beijingä¸ºç©ºï¼Œå¯èƒ½å¯¼è‡´æ•°æ®åº“å”¯ä¸€çº¦æŸå¤±è´¥")
            continue  # è·³è¿‡ç©ºæ—¶é—´çš„æ¯”èµ›ï¼ˆæ ¹æ®éœ€æ±‚å¤„ç†ï¼‰

        time_until_start = source1_raw_match.get('time_until_start', '') if source1_raw_match else ''

        # å°†è¯¥æ¯”èµ›çš„æ•°æ®æ·»åŠ åˆ°æ€»æ•°æ®ä¸­
        all_matches_data[match_name] = {
            "league_name": team_mapping['league']['source2'],
            "home_team": team_mapping['home']['source2'],
            "away_team": team_mapping['away']['source2'],
            "start_time_beijing": start_time_beijing,
            "time_until_start": time_until_start,
            "sources": source_data
        }

    return all_matches_data


# === æ–°å¢ï¼šè®¡ç®—èµ”ç‡æ•°æ®å“ˆå¸Œ ===
def calculate_odds_hash(match_data: Dict) -> str:
    """è®¡ç®—æ¯”èµ›èµ”ç‡æ•°æ®çš„MD5å“ˆå¸Œå€¼"""
    if not match_data:
        return ""

    # æå–å¹¶åºåˆ—åŒ–èµ”ç‡æ•°æ®
    odds_data = {}
    for source in match_data.get("sources", []):
        source_id = source.get("source")
        odds_data[source_id] = source.get("odds", {})

    # è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²å¹¶æ’åºé”®ï¼Œç¡®ä¿ç›¸åŒèµ”ç‡ç”Ÿæˆç›¸åŒå“ˆå¸Œ
    odds_str = json.dumps(odds_data, sort_keys=True, ensure_ascii=False, default=str).encode('utf-8')
    return md5(odds_str).hexdigest()


# === æ–°å¢ï¼šå¯¹æ¯”ä¸¤ä¸ªæ¯”èµ›çš„èµ”ç‡ï¼Œæ‰¾å‡ºå…·ä½“å˜åŒ– ===
def compare_odds(old_data: Dict, new_data: Dict) -> List[Dict]:
    """å¯¹æ¯”ä¸¤ä¸ªæ¯”èµ›çš„èµ”ç‡ï¼Œè¿”å›è¯¦ç»†å˜åŒ–åˆ—è¡¨ï¼ˆä¿ç•™Noneå€¼çš„æ¯”è¾ƒï¼‰"""
    changes = []

    # æŒ‰sourceåˆ†ç»„æ¯”è¾ƒ
    old_sources = {s["source"]: s["odds"] for s in old_data.get("sources", [])}
    new_sources = {s["source"]: s["odds"] for s in new_data.get("sources", [])}

    # éå†æ‰€æœ‰æ•°æ®æº
    for source_id in set(old_sources.keys()) | set(new_sources.keys()):
        old_odds = old_sources.get(source_id, {})
        new_odds = new_sources.get(source_id, {})

        # æ¯”è¾ƒè®©åˆ†ç›˜
        for spread_key in set(old_odds.get("spreads", {}).keys()) | set(new_odds.get("spreads", {}).keys()):
            old_spread = old_odds.get("spreads", {}).get(spread_key, {})
            new_spread = new_odds.get("spreads", {}).get(spread_key, {})

            for side in set(old_spread.keys()) | set(new_spread.keys()):
                old_value = old_spread.get(side)
                new_value = new_spread.get(side)

                # ä¿®æ”¹ç‚¹ï¼šä¿ç•™Noneå€¼çš„æ¯”è¾ƒï¼Œåªè¦æ–°æ—§å€¼ä¸åŒå°±è®°å½•å˜åŒ–
                if old_value != new_value:
                    changes.append({
                        "type": "spread",
                        "source": source_id,
                        "spread": spread_key,
                        "side": side,
                        "old_value": old_value,
                        "new_value": new_value
                    })

        # æ¯”è¾ƒå¤§å°çƒ
        for total_key in set(old_odds.get("totals", {}).keys()) | set(new_odds.get("totals", {}).keys()):
            old_total = old_odds.get("totals", {}).get(total_key, {})
            new_total = new_odds.get("totals", {}).get(total_key, {})

            for side in set(old_total.keys()) | set(new_total.keys()):
                old_value = old_total.get(side)
                new_value = new_total.get(side)

                # ä¿®æ”¹ç‚¹ï¼šä¿ç•™Noneå€¼çš„æ¯”è¾ƒï¼Œåªè¦æ–°æ—§å€¼ä¸åŒå°±è®°å½•å˜åŒ–
                if old_value != new_value:
                    changes.append({
                        "type": "total",
                        "source": source_id,
                        "total": total_key,
                        "side": side,
                        "old_value": old_value,
                        "new_value": new_value
                    })

    return changes


# === æ–°å¢ï¼šæ£€æŸ¥APIå“åº”æ˜¯å¦æœ‰å¤±è´¥ ===
def check_api_failures(results: List[Dict[str, Any]]) -> bool:
    """æ£€æŸ¥APIè¯·æ±‚ç»“æœä¸­æ˜¯å¦æœ‰å¤±è´¥çš„æƒ…å†µ"""
    failed_apis = [result["url"] for result in results if result["status"] == "error"]
    if failed_apis:
        print(f"â— æ£€æµ‹åˆ°APIå¤±æ•ˆ: {', '.join(failed_apis)}")
        return True
    return False


# === æ–°å¢ï¼šç»´æŠ¤å…¨å±€æ¯”èµ›æ•°æ®ç¼“å­˜ ===
def update_matches_cache(matches_data: Dict):
    """æ›´æ–°å…¨å±€æ¯”èµ›æ•°æ®ç¼“å­˜"""
    global all_matches_cache

    # å…ˆæ¸…é™¤å·²ä¸å­˜åœ¨çš„æ¯”èµ›
    current_matches = set(matches_data.keys())
    old_matches = set(all_matches_cache.keys())
    removed_matches = old_matches - current_matches

    for match_name in removed_matches:
        del all_matches_cache[match_name]

    # æ›´æ–°æˆ–æ·»åŠ æ¯”èµ›æ•°æ®
    for match_name, data in matches_data.items():
        # æ·»åŠ æ›´æ–°æ—¶é—´æˆ³
        data_with_timestamp = {
            **data,
            "last_updated": datetime.now().isoformat()
        }
        all_matches_cache[match_name] = data_with_timestamp

    print(f"âœ… æ¯”èµ›æ•°æ®ç¼“å­˜å·²æ›´æ–°ï¼Œå½“å‰ç¼“å­˜å¤§å°: {len(all_matches_cache)}")


# === æ–°å¢ï¼šWebSocketå¹¿æ’­å‡½æ•°ï¼ˆä¿®æ”¹ä¸ºéé˜»å¡ï¼‰===
async def broadcast_matches_data():
    """å®šæœŸå¹¿æ’­æœ€æ–°æ¯”èµ›æ•°æ®ç»™æ‰€æœ‰è¿æ¥çš„å®¢æˆ·ç«¯"""
    while True:
        try:
            if connected_clients and all_matches_cache:
                data_to_send = {
                    "timestamp": datetime.now().isoformat(),
                    "matches": list(all_matches_cache.values())
                }
                await asyncio.gather(
                    *[client.send(json.dumps(data_to_send)) for client in connected_clients]
                )
        except Exception as e:
            print(f"âŒ WebSocketå¹¿æ’­å¤±è´¥: {e}")
        await asyncio.sleep(5)  # æ¯5ç§’å¹¿æ’­ä¸€æ¬¡


async def ws_handler(websocket, path):
    """å¤„ç†WebSocketè¿æ¥"""
    # æ·»åŠ å®¢æˆ·ç«¯åˆ°è¿æ¥é›†åˆ
    connected_clients.add(websocket)
    print(f"âœ… æ–°çš„WebSocketè¿æ¥ï¼Œå½“å‰è¿æ¥æ•°: {len(connected_clients)}")

    try:
        # ä¿æŒè¿æ¥æ‰“å¼€
        await websocket.wait_closed()
    finally:
        # è¿æ¥å…³é—­æ—¶ç§»é™¤å®¢æˆ·ç«¯
        connected_clients.remove(websocket)
        print(f"â„¹ï¸ WebSocketè¿æ¥å·²å…³é—­ï¼Œå½“å‰è¿æ¥æ•°: {len(connected_clients)}")
# === ä¸»å‡½æ•° ===
@timed
async def main():
    """ä¸»å‡½æ•°ï¼šå‘¨æœŸæ€§è·å–æ‰€æœ‰APIæ•°æ®å¹¶é€šè¿‡WebSocketæ¨é€æ›´æ–°"""
    global postgres_pool, last_matches_data

    # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± å’Œè¡¨
    if not init_db_pool() or not init_db_tables():
        print("âŒ æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
        return

    try:
        print(f"\n{'=' * 20} ç¨‹åºå¯åŠ¨ï¼Œè·å–åˆå§‹æ•°æ® [{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {'=' * 20}")

        # === WebSocket æœåŠ¡å¯åŠ¨ ===
        ws_server = await websockets.serve(ws_handler, WS_CONFIG["host"], WS_CONFIG["port"])
        print(f"âœ… WebSocketæœåŠ¡å·²å¯åŠ¨: ws://{WS_CONFIG['host']}:{WS_CONFIG['port']}")

        # é¦–æ¬¡æ•°æ®è·å–ä¸åˆå§‹åŒ–
        async with aiohttp.ClientSession() as session:
            tasks = [fetch_api(session, url) for url in API_URLS]
            results = await asyncio.gather(*tasks)

            if check_api_failures(results):
                print(f"âš ï¸ ç¨‹åºå°†æš‚åœ {API_FAILURE_DELAY} ç§’åç»§ç»­è¿è¡Œ...")
                await asyncio.sleep(API_FAILURE_DELAY)
                return

            all_matches_data = await process_api_data(results)

        if all_matches_data:
            print("\n" + "=" * 50)
            print(f"ğŸ“¥ åˆå§‹åŒ–ï¼šä¿å­˜åˆå§‹æ¯”èµ›æ•°æ®åˆ°æ•°æ®åº“")
            print("=" * 50)

            for match_name, match_data in all_matches_data.items():
                match_id = save_match_info(match_name, match_data)
                if match_id:
                    dummy_changes = []
                    for source in match_data.get("sources", []):
                        source_id = source.get("source")
                        for spread_key, spread_data in source.get("odds", {}).get("spreads", {}).items():
                            for side, value in spread_data.items():
                                dummy_changes.append({
                                    "type": "spread",
                                    "source": source_id,
                                    "spread": spread_key,
                                    "side": side,
                                    "old_value": None,
                                    "new_value": value
                                })
                        for total_key, total_data in source.get("odds", {}).get("totals", {}).items():
                            for side, value in total_data.items():
                                dummy_changes.append({
                                    "type": "total",
                                    "source": source_id,
                                    "total": total_key,
                                    "side": side,
                                    "old_value": None,
                                    "new_value": value
                                })
                    if dummy_changes:
                        save_odds_changes(match_id, match_name, dummy_changes)
                        print(f"âœ… å·²ä¿å­˜åˆå§‹æ•°æ®: {match_name} ({len(dummy_changes)} æ¡èµ”ç‡è®°å½•)")

            for match_name, match_data in all_matches_data.items():
                cache_key = (match_name, match_data["start_time_beijing"])
                last_matches_data[cache_key] = (calculate_odds_hash(match_data), match_data)

            update_matches_cache(all_matches_data)

            # å°†å¹¿æ’­å‡½æ•°ä½œä¸ºç‹¬ç«‹ä»»åŠ¡è¿è¡Œï¼Œä¸é˜»å¡ä¸»å¾ªç¯
            broadcast_task = asyncio.create_task(broadcast_matches_data())

            print(f"\nâœ… åˆå§‹æ•°æ®ä¿å­˜å®Œæˆï¼Œå…± {len(all_matches_data)} åœºæ¯”èµ›")
        else:
            print("â„¹ï¸ åˆå§‹æ•°æ®ä¸ºç©ºï¼Œç¨‹åºå°†ç»§ç»­è¿è¡Œä½†æ— æ•°æ®å¯ä¿å­˜")

        # ä¸»å¾ªç¯ï¼šå‘¨æœŸæ€§æ•°æ®å¤„ç†
        print(f"\n{'=' * 20} å¼€å§‹å‘¨æœŸæ€§æ•°æ®è·å– [{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {'=' * 20}")

        while True:
            try:
                start_time = time.time()
                print(f"\n{'=' * 20} å¼€å§‹æ–°ä¸€è½®æ•°æ®è·å– [{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {'=' * 20}")

                # è·å–æœ€æ–°æ•°æ®
                async with aiohttp.ClientSession() as session:
                    tasks = [fetch_api(session, url) for url in API_URLS]
                    results = await asyncio.gather(*tasks)

                if check_api_failures(results):
                    print(f"âš ï¸ æ£€æµ‹åˆ°APIè¯·æ±‚å¤±è´¥ï¼Œè·³è¿‡æ­¤è½®æ•°æ®å¤„ç†")
                    await asyncio.sleep(API_FAILURE_DELAY)
                    continue

                all_matches_data = await process_api_data(results)

                if not all_matches_data:
                    print("â„¹ï¸ æœ¬è½®è·å–çš„æ¯”èµ›æ•°æ®ä¸ºç©º")
                    await asyncio.sleep(FETCH_INTERVAL)
                    continue

                # æ•°æ®å¯¹æ¯”ä¸å˜åŒ–æ£€æµ‹
                new_matches = []  # æ–°å¢æ¯”èµ›
                changed_matches = []  # èµ”ç‡å˜åŒ–çš„æ¯”èµ›
                removed_matches = []  # ç§»é™¤çš„æ¯”èµ›
                detailed_changes = {}  # è¯¦ç»†èµ”ç‡å˜åŒ–

                # ä½¿ç”¨match_name + start_time_beijingä½œä¸ºå”¯ä¸€æ ‡è¯†
                current_cache_keys = {(match_name, data["start_time_beijing"]) for match_name, data in
                                      all_matches_data.items()}
                previous_cache_keys = set(last_matches_data.keys())

                # æ£€æŸ¥æ–°å¢æ¯”èµ›
                for cache_key in current_cache_keys - previous_cache_keys:
                    match_name, _ = cache_key
                    if match_name in all_matches_data:
                        new_matches.append(match_name)
                        current_data = all_matches_data[match_name]
                        current_hash = calculate_odds_hash(current_data)
                        last_matches_data[cache_key] = (current_hash, current_data)

                        # ä¿å­˜æ–°æ¯”èµ›åˆ°æ•°æ®åº“
                        match_id = save_match_info(match_name, current_data)
                        if match_id:
                            # æå–æ‰€æœ‰èµ”ç‡ä½œä¸º"å˜åŒ–"ä¿å­˜
                            initial_changes = []
                            for source in current_data.get("sources", []):
                                source_id = source.get("source")
                                for spread_key, spread_data in source.get("odds", {}).get("spreads", {}).items():
                                    for side, value in spread_data.items():
                                        initial_changes.append({
                                            "type": "spread",
                                            "source": source_id,
                                            "spread": spread_key,
                                            "side": side,
                                            "old_value": None,
                                            "new_value": value
                                        })
                                for total_key, total_data in source.get("odds", {}).get("totals", {}).items():
                                    for side, value in total_data.items():
                                        initial_changes.append({
                                            "type": "total",
                                            "source": source_id,
                                            "total": total_key,
                                            "side": side,
                                            "old_value": None,
                                            "new_value": value
                                        })
                            if initial_changes:
                                save_odds_changes(match_id, match_name, initial_changes)
                                print(f"âœ… å·²ä¿å­˜æ–°æ¯”èµ›æ•°æ®: {match_name} ({len(initial_changes)} æ¡èµ”ç‡è®°å½•)")

                # æ£€æŸ¥èµ”ç‡å˜åŒ–
                for cache_key in current_cache_keys & previous_cache_keys:
                    match_name, _ = cache_key
                    current_data = all_matches_data[match_name]
                    current_hash = calculate_odds_hash(current_data)
                    previous_hash, previous_data = last_matches_data[cache_key]

                    if current_hash != previous_hash:
                        changed_matches.append(match_name)
                        last_matches_data[cache_key] = (current_hash, current_data)

                        # è®¡ç®—è¯¦ç»†å˜åŒ–
                        changes = compare_odds(previous_data, current_data)
                        if changes:
                            detailed_changes[match_name] = changes

                            # ä¿å­˜å˜åŒ–åˆ°æ•°æ®åº“
                            match_id = save_match_info(match_name, current_data)
                            if match_id:
                                save_odds_changes(match_id, match_name, changes)

                # æ£€æŸ¥ç§»é™¤çš„æ¯”èµ›
                for cache_key in previous_cache_keys - current_cache_keys:
                    match_name, _ = cache_key
                    if cache_key in last_matches_data:
                        removed_matches.append(match_name)
                        del last_matches_data[cache_key]

                # æ›´æ–°å…¨å±€ç¼“å­˜
                update_matches_cache(all_matches_data)

                # æ‰“å°å˜åŒ–ç»Ÿè®¡
                print("\n" + "=" * 50)
                print(f"ğŸ“Š æ•°æ®å˜åŒ–ç»Ÿè®¡")
                print("=" * 50)
                print(f"  - æ–°å¢æ¯”èµ›: {len(new_matches)}")
                print(f"  - èµ”ç‡å˜åŒ–: {len(changed_matches)}")
                print(f"  - ç§»é™¤æ¯”èµ›: {len(removed_matches)}")

                # æ‰“å°æ–°å¢æ¯”èµ›
                if new_matches:
                    print("\nğŸ“ˆ æ–°å¢æ¯”èµ›:")
                    for match_name in new_matches:
                        print(f"  - {match_name}")

                # æ‰“å°è¯¦ç»†èµ”ç‡å˜åŒ–
                if detailed_changes:
                    print("\nğŸ“Š è¯¦ç»†èµ”ç‡å˜åŒ–:")
                    for match_name, changes in detailed_changes.items():
                        print(f"\n  - {match_name}")
                        for change in changes:
                            if change["type"] == "spread":
                                print(
                                    f"    ğŸ”¹ æ•°æ®æº{change['source']} è®©åˆ†ç›˜ {change['spread']} - {change['side']}: {change['old_value']} â†’ {change['new_value']}")
                            else:
                                print(
                                    f"    ğŸ”¹ æ•°æ®æº{change['source']} å¤§å°çƒ {change['total']} - {change['side']}: {change['old_value']} â†’ {change['new_value']}")

                # æ‰“å°ç§»é™¤æ¯”èµ›
                if removed_matches:
                    print("\nâŒ ç§»é™¤æ¯”èµ›:")
                    for match_name in removed_matches:
                        print(f"  - {match_name}")

                if not new_matches and not changed_matches and not removed_matches:
                    print("\nâ„¹ï¸ æ— æ•°æ®å˜åŒ–")

                # è®¡ç®—å¤„ç†æ—¶é—´å’Œä¸‹ä¸€æ¬¡è·å–æ—¶é—´
                elapsed = time.time() - start_time
                next_fetch_in = max(0, FETCH_INTERVAL - elapsed)
                print(f"\n{'=' * 50}")
                print(f"ğŸ“Š æœ¬è½®æ•°æ®å¤„ç†å®Œæˆ")
                print(f"  - å¤„ç†æ—¶é—´: {elapsed:.2f}ç§’")
                print(f"  - ä¸‹æ¬¡æ•°æ®è·å–å°†åœ¨ {next_fetch_in:.2f}ç§’åè¿›è¡Œ")
                print(f"{'=' * 50}\n")

                # ç­‰å¾…ä¸‹ä¸€ä¸ªå‘¨æœŸ
                await asyncio.sleep(next_fetch_in)

            except Exception as e:
                print(f"âŒ å‘¨æœŸæ•°æ®è·å–å¼‚å¸¸: {e}")
                # è®°å½•å®Œæ•´å †æ ˆè·Ÿè¸ª
                import traceback
                traceback.print_exc()
                # ç­‰å¾…ä¸€æ®µæ—¶é—´å†é‡è¯•
                await asyncio.sleep(FETCH_INTERVAL)

    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç”¨æˆ·æ‰‹åŠ¨ç»ˆæ­¢ç¨‹åº")
    finally:
        # èµ„æºæ¸…ç†
        if 'ws_server' in locals():
            ws_server.close()
            await ws_server.wait_closed()

        # å–æ¶ˆå¹¿æ’­ä»»åŠ¡
        if 'broadcast_task' in locals():
            broadcast_task.cancel()
            await broadcast_task

        # å…³é—­æ•°æ®åº“è¿æ¥æ± 
        if postgres_pool:
            postgres_pool.closeall()
            print("âœ… æ•°æ®åº“è¿æ¥æ± å·²å…³é—­")

        print("ğŸ‘‹ ç¨‹åºå·²é€€å‡º")


if __name__ == "__main__":
    if hasattr(asyncio, 'WindowsSelectorEventLoopPolicy'):
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

    asyncio.run(main())